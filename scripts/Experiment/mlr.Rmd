---
title: "mlr"
author: "Xiaochi"
date: "06/02/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = TRUE,
	warning = TRUE
)
library(mlr)
library(tidyverse)
```

## Task

```{r}
data(BostonHousing, package = "mlbench")
names(BostonHousing)
regr.task = makeRegrTask(id = "bh", data = BostonHousing, target = "medv")
```

```{r}
data(BreastCancer, package = "mlbench")
df = BreastCancer
df$Id = NULL
classif.task = makeClassifTask(id = "BreastCancer", 
                               data = df, 
                               target = "Class")
classif.task
```

```{r}
getTaskDesc(classif.task)
```

## Learners

```{r}
# Classification tree, set it up for predicting probabilities
classif.lrn = makeLearner("classif.randomForest", 
                          predict.type = "prob", 
                          fix.factors.prediction = TRUE)

# Regression gradient boosting machine, specify hyperparameters via a list
regr.lrn = makeLearner("regr.gbm", 
                       par.vals = list(n.trees = 500, 
                                       interaction.depth = 3))

# Cox proportional hazards model with custom name
surv.lrn = makeLearner("surv.coxph", id = "cph")

# K-means with 5 clusters
cluster.lrn = makeLearner("cluster.kmeans", centers = 5)

# Multilabel Random Ferns classification algorithm
multilabel.lrn = makeLearner("multilabel.rFerns")
```

```{r}
classif.lrn
surv.lrn
regr.lrn
regr.lrn$par.set
getHyperPars(regr.lrn)
getLearnerParVals(regr.lrn)
getParamSet(regr.lrn)
getLearnerParamSet(regr.lrn)
getParamSet("classif.randomForest")
```

```{r}
surv.lrn
surv.lrn = setLearnerId(surv.lrn, "CoxModel")
surv.lrn
```



```{r}
listLearners()
```

### Training a Learner

```{r}
# Generate the task
task = makeClassifTask(data = iris, target = "Species")

# Generate the learner
lrn = makeLearner("classif.lda")

# Train the learner
mod = train(lrn, task)
mod
```

```{r}
mod = train("classif.lda", task)
mod
```


```{r}
mod = train("surv.coxph", lung.task)
mod
```
### Accessing learner models

```{r}
data(ruspini, package = "cluster")
plot(y ~ x, ruspini)
```

```{r}
# Generate the task
ruspini.task = makeClusterTask(data = ruspini)

# Generate the learner
lrn = makeLearner("cluster.kmeans", centers = 4)

# Train the learner
mod = train(lrn, ruspini.task)
mod

# Peak into mod
names(mod)
mod$learner
mod$features
getLearnerModel(mod)
```

### Further options and comments

```{r}
# Get the number of observations
n = getTaskSize(bh.task)

# Use 1/3 of the observations for training
train.set = sample(n, size = n/3)

# Train the learner
mod = train("regr.lm", bh.task, subset = train.set)
mod
```

```{r}
# Calculate the observation weights
target = getTaskTargets(bc.task)
tab = as.numeric(table(target))
w = 1/tab[target]

train("classif.rpart", task = bc.task, weights = w)
```



```{r}
mod$learner.model
getLearnerModel(mod)

```

```{r}
data(ruspini, package = "cluster")
plot(y ~ x, ruspini)
```

```{r}
# Generate the task
ruspini.task = makeClusterTask(data = ruspini)

# Generate the learner
lrn = makeLearner("cluster.kmeans", centers = 4)

# Train the learner
mod = train(lrn, ruspini.task)
mod
```

```{r}
names(mod)
mod$learner
getLearnerModel(mod)
mod$features

```

```{r}
# Get the number of observations
n = getTaskSize(bh.task)

# Use 1/3 of the observations for training
train.set = sample(n, size = n/3)

# Train the learner
mod = train("regr.lm", bh.task, subset = train.set)
mod
```

### Wrapper

```{r}
data(iris)
task = makeClassifTask(data = iris, 
                       target = "Species", 
                       weights = as.integer(iris$Species))

# make learner
base.lrn = makeLearner("classif.rpart")

# make bagging wrapper
wrapped.lrn = makeBaggingWrapper(base.lrn, bw.iters = 100, bw.feats = 0.5)

benchmark(tasks = task, learners = list(base.lrn, wrapped.lrn))

# make tunning warpper
getParamSet(wrapped.lrn)
ctrl = makeTuneControlRandom(maxit = 10)
rdesc = makeResampleDesc("CV", iters = 3)
par.set = makeParamSet(
  makeIntegerParam("minsplit", lower = 1, upper = 10),
  makeNumericParam("bw.feats", lower = 0.25, upper = 1)
)
tuned.lrn = makeTuneWrapper(learner = wrapped.lrn, 
                            resampling = rdesc, 
                            measures = mmce, 
                            par.set = par.set, 
                            control = ctrl,
                            show.info = TRUE)
tuned.lrn$next.learner
wrapped.lrn$next.learner

lrn = train(tuned.lrn, task = task)
```

## Preprocessing

```{r}
sonar.task
lrn = makePreprocWrapperCaret("classif.qda", ppc.pca = TRUE, ppc.thresh = 0.9)
lrn

mod = mlr::train(lrn, sonar.task)
mod

getLearnerModel(mod)
getLearnerModel(mod, more.unwrap = TRUE)

rin = makeResampleInstance("CV", iters = 3, stratify = TRUE, task = sonar.task)
res = benchmark(list("classif.qda", lrn), sonar.task, rin, show.info = FALSE)
res

getParamSet(lrn)

ps = makeParamSet(
  makeIntegerParam("ppc.pcaComp", lower = 1, upper = getTaskNFeats(sonar.task)),
  makeDiscreteParam("predict.method", values = c("plug-in", "debiased"))
)
ctrl = makeTuneControlGrid(resolution = 10)
res = tuneParams(lrn, 
                 sonar.task, 
                 rin, 
                 par.set = ps, 
                 control = ctrl, 
                 show.info = TRUE)
res
res$opt.path %>% 
  as.data.frame()
```

## Imputation


```{r}
data(airquality)
summary(airquality)
```

```{r}
airq = airquality
ind = sample(nrow(airq), 10)
airq$Wind[ind] = NA
airq$Wind = cut(airq$Wind, c(0,8,16,24))
summary(airq)
```

```{r}
imp = impute(airq, 
             classes = list(integer = imputeMean(), 
                            factor = imputeMode()),
             dummy.classes = "integer")

imp$data
imp$desc
```



```{r}
airq = subset(airq, select = 1:4)

airq.train = airq[1:100,]
airq.test = airq[-c(1:100),]

summary(airq.train)
summary(airq.test)

imp = impute(airq.train, 
             target = "Ozone", 
             cols = list(Solar.R = imputeHist(), 
                         Wind = imputeLearner("classif.rpart")), 
             dummy.cols = c("Solar.R", "Wind"))

imp$data
imp$desc

airq.test.imp = reimpute(airq.test, imp$desc)
airq.test.imp

lrn = makeImputeWrapper("regr.lm", 
                        cols = list(Solar.R = imputeHist(),
                                    Wind = imputeLearner("classif.rpart")),
                        dummy.cols = c("Solar.R", "Wind"))

lrn

airq = subset(airq, subset = !is.na(airq$Ozone))
task = makeRegrTask(data = airq, target = "Ozone")

rdesc = makeResampleDesc("CV", iters = 3)
r = resample(lrn, 
             task, 
             resampling = rdesc, 
             show.info = FALSE, 
             models = TRUE)
r$aggr

lapply(r$models, getLearnerModel, more.unwrap = TRUE)

```


## Benchmark

```{r}
data(BostonHousing, package = "mlbench")
summary(BostonHousing)

# Make a task
regr.task = makeRegrTask(data = BostonHousing, target = "medv")
regr.task

listLearners(regr.task)
getLearnerp

set.seed(1234)

# Define a search space for each learner's parameter
ps_ksvm = makeParamSet(
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x)
)

ps_rf = makeParamSet(
  makeIntegerParam("num.trees", lower = 1L, upper = 200L)
)

# Choose a resampling strategy
rdesc = makeResampleDesc("CV", iters = 5L)

# Choose a performance measure
meas = rmse

# Choose a tuning method
ctrl = makeTuneControlCMAES(budget = 100L)

# Make tuning wrappers
tuned.ksvm = makeTuneWrapper(learner = "regr.ksvm", 
                             resampling = rdesc, 
                             measures = meas,
                             par.set = ps_ksvm, 
                             control = ctrl, 
                             show.info = FALSE)

tuned.rf = makeTuneWrapper(learner = "regr.ranger", 
                           resampling = rdesc, 
                           measures = meas,
                           par.set = ps_rf, 
                           control = ctrl, 
                           show.info = FALSE)

# 3 learners to be compared
lrns = list(makeLearner("regr.lm"), tuned.ksvm, tuned.rf)

# Conduct the benchmark experiment
bmr = benchmark(learners = lrns, 
                tasks = regr.task, 
                resamplings = rdesc, 
                measures = rmse, 
                show.info = FALSE)

plotBMRBoxplots(bmr)
```

```{r}
# Two learners to be compared
lrns = list(makeLearner("classif.lda"), 
            makeLearner("classif.rpart"))
# Vector of strings
lrns = c("classif.lda", "classif.rpart")
# A mixed list of Learner objects and strings works, too
lrns = list(makeLearner("classif.lda", predict.type = "prob"), 
            "classif.rpart")


# Choose the resampling strategy
rdesc = makeResampleDesc("Holdout")

# Conduct the benchmark experiment
bmr = benchmark(lrns, 
                sonar.task, 
                rdesc,
                models = TRUE)

bmr

getBMRPerformances(bmr)
getBMRAggrPerformances(bmr)
getBMRPerformances(bmr, drop = TRUE)
getBMRPerformances(bmr, as.df = TRUE)
getBMRAggrPerformances(bmr, as.df = TRUE)

getBMRPredictions(bmr)
getBMRPredictions(bmr, as.df = TRUE)
getBMRPredictions(bmr, learner.ids = "classif.rpart", as.df = TRUE)

getBMRTaskIds(bmr)
getBMRLearnerIds(bmr)
getBMRMeasureIds(bmr)

getBMRModels(bmr)
getBMRModels(bmr, drop = TRUE)
getBMRModels(bmr, learner.ids = "classif.lda")

getBMRLearners(bmr)
getBMRMeasures(bmr)
```
```{r}
# First benchmark result
bmr

# Benchmark experiment for the additional learners
lrns2 = list(makeLearner("classif.randomForest"), 
             makeLearner("classif.qda"))

bmr2 = mlr::benchmark(lrns2, 
                 sonar.task, 
                 rdesc, 
                 show.info = FALSE)
bmr2

# Merge the results
mergeBenchmarkResults(list(bmr, bmr2))
```

```{r}
rin = getBMRPredictions(bmr)[[1]][[1]]$instance
rin

# Benchmark experiment for the additional random forest
bmr3 = benchmark(lrns2, sonar.task, rin, show.info = FALSE)
bmr3

# Merge the results
mergeBenchmarkResults(list(bmr, bmr3))
```

```{r}
library(mlbench)

# Create a list of learners
lrns = list(
  makeLearner("classif.lda", id = "lda"),
  makeLearner("classif.rpart", id = "rpart"),
  makeLearner("classif.randomForest", id = "randomForest")
)

# Get additional Tasks from package mlbench
ring.task = convertMLBenchObjToTask("mlbench.ringnorm", n = 600)
wave.task = convertMLBenchObjToTask("mlbench.waveform", n = 600)

tasks = list(iris.task, sonar.task, pid.task, ring.task, wave.task)
rdesc = makeResampleDesc("CV", iters = 10)
meas = list(mmce, ber, timetrain)
bmr = benchmark(lrns, tasks, rdesc, meas, show.info = TRUE)

perf = getBMRPerformances(bmr, as.df = TRUE)
perf[, -ncol(perf)]
```

```{r}
plotBMRBoxplots(bmr, measure = mmce, order.lrn = getBMRLearnerIds(bmr))
```
```{r}
plotBMRBoxplots(bmr, 
                measure = ber, 
                style = "violin", 
                pretty.names = FALSE,
                order.lrn = getBMRLearnerIds(bmr)) +
  aes(color = learner.id) +
  theme(strip.text.x = element_text(size = 8))
```
```{r}
mmce$name
mmce$id
ber$name
getBMRLearnerIds(bmr)
getBMRLearnerShortNames(bmr)
```

```{r}
plt = plotBMRBoxplots(bmr, measure = mmce, order.lrn = getBMRLearnerIds(bmr))
plt$data

levels(plt$data$task.id) = c("Iris", "Ringnorm", "Waveform", "Diabetes", "Sonar")
levels(plt$data$learner.id) = c("LDA", "CART", "RF")

plt + ylab("Error rate")
```



## Predicting

```{r}
n = getTaskSize(bh.task)
train.set = seq(1, n, by = 2)
test.set = seq(2, n, by = 2)
lrn = makeLearner("regr.gbm", n.trees = 100)

mod = train(lrn, 
            bh.task, 
            subset = train.set)

task.pred = predict(mod, 
                    task = bh.task, 
                    subset = test.set)
task.pred
```

```{r}
n = nrow(iris)
iris.train = iris[seq(1, n, by = 2), -5]
iris.test = iris[seq(2, n, by = 2), -5]

task = makeClusterTask(data = iris.train)

mod = train("cluster.kmeans", task)

newdata.pred = predict(mod, newdata = iris.test)
newdata.pred
```


### Accessing the prediction

```{r}
### Result of predict with data passed via task argument
as.data.frame(task.pred)
```

```{r}
### Result of predict with data passed via newdata argument
as.data.frame(newdata.pred)
```

```{r}
getPredictionTruth(task.pred)
getPredictionResponse(task.pred)
```

### Regression: Extracting standard errors

```{r}
listLearners("regr", check.packages = FALSE, properties = "se")[c("class", "name")]
```

```{r}
### Create learner and specify predict.type
lrn.lm = makeLearner("regr.lm", predict.type = 'se')
mod.lm = train(lrn.lm, bh.task, subset = train.set)
task.pred.lm = predict(mod.lm, task = bh.task, subset = test.set)
task.pred.lm
getPredictionSE(task.pred.lm)
```

### Classification and clustering: Extracting probabilities

```{r}
lrn = makeLearner("cluster.cmeans", predict.type = "prob")
mod = train(lrn, mtcars.task)
pred = predict(mod, task = mtcars.task)
getPredictionProbabilities(pred)
```

```{r}
### Linear discriminate analysis on the iris data set
mod = train("classif.lda", task = iris.task)
pred = predict(mod, task = iris.task)
```

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, iris.task)
pred = predict(mod, newdata = iris)
getPredictionProbabilities(pred)
```

### Classification: Confusion matrix

```{r}
calculateConfusionMatrix(pred)
calculateConfusionMatrix(pred, relative = TRUE)
calculateConfusionMatrix(pred, relative = TRUE)$relative.row
calculateConfusionMatrix(pred, relative = TRUE)$relative.col
calculateConfusionMatrix(pred, relative = TRUE, sums = TRUE)
```

### Classification: Adjusting the decision threshold

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")

mod = train(lrn, task = sonar.task)

### Label of the positive class
getTaskDesc(sonar.task)$positive

### Default threshold
pred1 = predict(mod, sonar.task)
pred1$threshold

### Set the threshold value for the positive class
pred2 = setThreshold(pred1, 0.9)
pred2$threshold
pred2

### We can also set the effect in the confusion matrix
calculateConfusionMatrix(pred1)

calculateConfusionMatrix(pred2)
```

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, iris.task)
pred = predict(mod, newdata = iris)
pred$threshold
table(as.data.frame(pred)$response)
```

### Visualizing the prediction

```{r}
lrn = makeLearner("classif.rpart", id = "CART")
plotLearnerPrediction(lrn, task = iris.task)
```

```{r}
lrn = makeLearner("cluster.kmeans")
plotLearnerPrediction(lrn, task = mtcars.task, features = c("disp", "drat"), cv = 0)
```

```{r}
plotLearnerPrediction("regr.lm", features = "lstat", task = bh.task)
```

```{r}
plotLearnerPrediction("regr.lm", features = c("lstat", "rm"), task = bh.task)
```



## Tuning Hyperparameters

### search space

```{r}
# ex: create a search space for the C hyperparameter from 0.01 to 0.1
ps = makeParamSet(
  makeNumericParam("C", lower = 0.01, upper = 0.1)
)

# ex: random search with 100 iterations
ctrl = makeTuneControlRandom(maxit = 100L)

rdesc = makeResampleDesc("CV", iters = 3L)
measure = acc
```


```{r}
discrete_ps = makeParamSet(makeDiscreteParam("C", values = c(0.5, 1.0, 1.5, 2.0)),
                           makeDiscreteParam("sigma", values = c(0.5, 1.0, 1.5, 2.0)))
discrete_ps
```


```{r}
num_ps = makeParamSet(makeNumericParam("C", lower = -10, upper = 10, trafo = function(x) 10^x),
                      makeNumericParam("sigma", lower = -10, upper = 10, trafo = function(x) 10^x))

num_ps
```

### optimization algorithm

```{r}
ctrl_grid = makeTuneControlGrid()
```

```{r}
ctrl_random = makeTuneControlRandom(maxit = 200L)
```

### performing the tuning

```{r}
rdesc = makeResampleDesc("CV", iters = 3L)
```

```{r}
discrete_ps = makeParamSet(makeDiscreteParam("C", values = c(0.5, 1.0, 1.5, 2.0)),
                           makeDiscreteParam("sigma", values = c(0.5, 1.0, 1.5, 2.0)))

ctrl = makeTuneControlGrid()

rdesc = makeResampleDesc("CV", iters = 3L)

res = tuneParams("classif.ksvm", 
                 task = iris.task, 
                 resampling = rdesc,
                 par.set = discrete_ps, 
                 control = ctrl)
```

```{r}
num_ps = makeParamSet(makeNumericParam("C", lower = -10, upper = 10, trafo = function(x) 10^x),
                      makeNumericParam("sigma", lower = -10, upper = 10, trafo = function(x) 10^x))

ctrl = makeTuneControlRandom(maxit = 100L)

res = tuneParams("classif.ksvm", 
                 task = iris.task, 
                 resampling = rdesc, 
                 par.set = num_ps,
                 control = ctrl, 
                 measures = list(acc, setAggregation(acc, test.sd)), 
                 show.info = TRUE)
```

### Accessing the tuning result

```{r}
res$x
res$y

lrn = setHyperPars(makeLearner("classif.ksvm"), 
                   C = res$x$C, 
                   sigma = res$x$sigma)


m = train(lrn, iris.task)

predict(m, task = iris.task)
```

### hyperparameter tuning effects

```{r}
generateHyperParsEffectData(res)
generateHyperParsEffectData(res, trafo = TRUE)
```

```{r}
rdesc2 = makeResampleDesc("Holdout", predict = "both")
res2 = tuneParams("classif.ksvm", 
                  task = iris.task, 
                  resampling = rdesc2, 
                  par.set = num_ps,
                  control = ctrl, 
                  measures = list(acc, setAggregation(acc, train.mean)), 
                  show.info = TRUE)
generateHyperParsEffectData(res2)
```

```{r}
res = tuneParams("classif.ksvm", 
                 task = iris.task, 
                 resampling = rdesc, 
                 par.set = num_ps,
                 control = ctrl, 
                 measures = list(acc), 
                 show.info = TRUE)

data = generateHyperParsEffectData(res)

plotHyperParsEffect(data, 
                    x = "iteration", 
                    y = "acc.test.mean",
                    plot.type = "line")
```

## Evaluating Hyperparameter Tuning

```{r}
# ex: create a search space for the C hyperparameter from 0.01 to 0.1
ps = makeParamSet(
  makeNumericParam("C", lower = 0.01, upper = 0.1)
)

# ex: random search with 100 iterations
ctrl = makeTuneControlRandom(maxit = 100L)

# ex: 2-fold CV
rdesc = makeResampleDesc("CV", iters = 2L)
```






```{r}
ps = makeParamSet(
  makeNumericParam("C", lower = -5, upper = 5, trafo = function(x) 2^x)
)

ctrl = makeTuneControlRandom(maxit = 200)

rdesc = makeResampleDesc("CV", iters = 5)

res = tuneParams("classif.ksvm", 
                 task = pid.task, 
                 control = ctrl,
                 measures = list(acc, mmce), 
                 resampling = rdesc, 
                 par.set = ps, 
                 show.info = TRUE)

generateHyperParsEffectData(res, trafo = T, include.diagnostics = FALSE)
```


### 3 or more hyperparameters

```{r}
# remotes::install_github("mlr-org/mlr")
library(mlr)

ps = makeParamSet(
  makeNumericParam("C", lower = -5, upper = 5, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -5, upper = 5, trafo = function(x) 2^x),
  makeDiscreteParam("degree", values = 2:5))

ctrl = makeTuneControlRandom(maxit = 100L)

rdesc = makeResampleDesc("Holdout", predict = "both")

learn = makeLearner("classif.ksvm", par.vals = list(kernel = "besseldot"))

res = tuneParams(learn, 
                 task = pid.task, 
                 control = ctrl,
                 measures = list(acc), 
                 resampling = rdesc,
                 par.set = ps, 
                 show.info = FALSE)

data = generateHyperParsEffectData(res, partial.dep = TRUE)

data

plotHyperParsEffect(data, x = "iteration", y = "acc.test.mean", 
                    plot.type = "line", partial.dep.learn = "regr.randomForest")

plotHyperParsEffect(data, x = "C", y = "sigma", z = "acc.test.mean",
                    plot.type = "heatmap", 
                    partial.dep.learn = "regr.randomForest")
```




## Resampling

### Defining the resampling strategy

```{r}
# 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)
rdesc

# Holdout estimation
rdesc = makeResampleDesc("Holdout")
rdesc
```
### Performing the resampling

```{r}
# Specify the resampling strategy (3-fold cross-validation)
rdesc = makeResampleDesc("CV", iters= 3)

# Calculate the performance
r = resample("regr.lm", bh.task, rdesc)

r
names(r)
r$measures.test
```

```{r}
# Subsampling with 5 iterations and default split ratio 2/3
rdesc = makeResampleDesc("Subsample", iters = 5)

# Subsampling with 5 iterations and 4/5 training data
rdesc = makeResampleDesc("Subsample", iters = 5, split = 4/5)

# Classification tree with information splitting criterion
lrn = makeLearner("classif.rpart", parms = list(split = "information"))

# Calculate the performance measures
r = resample(lrn, 
             sonar.task, 
             rdesc, 
             measures = list(mmce, fpr, fnr, timetrain))

r

# Add balanced error rate (ber) and time used to predict
addRRMeasure(r, list(ber, timepredict))
```
```{r}
r = resample("classif.rpart", 
             parms = list(split = "information"), 
             sonar.task, 
             rdesc,
             measures = list(mmce, fpr, fnr, timetrain), 
             show.info = TRUE)
```

### Accessing resample results

```{r}
r$pred
pred = getRRPredictions(r)
pred
as.data.frame(pred)
getPredictionTruth(pred)
getPredictionResponse(pred)
```

```{r}
# Make predictions on both training and test sets
rdesc = makeResampleDesc("Holdout", predict = "both")

r = resample("classif.lda", iris.task, rdesc, show.info = FALSE)
r
r$measures.train

predList = getRRPredictionList(r)
predList
```

### Learner models

```{r}
# 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)

r = resample("surv.coxph", 
             lung.task, 
             rdesc, 
             show.info = TRUE, 
             models = TRUE)

r$models
```

### The extract option

```{r}
# 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)

# Extract the compute cluster centers
r = resample("cluster.kmeans", 
             mtcars.task, 
             rdesc, 
             show.info = TRUE,
             centers = 3, 
             extract = function(x) getLearnerModel(x)$centers)

r$extract
```

```{r}
# Extract the variable importance in a regression tree
r = resample("regr.rpart", 
             bh.task, 
             rdesc, 
             show.info = TRUE, 
             extract = getFeatureImportance)

r$extract
```

### Stratification

```{r}
# 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3, stratify = TRUE)

r = resample("classif.lda", iris.task, rdesc, show.info = TRUE)
```

```{r}
rdesc = makeResampleDesc("CV", iters = 3, stratify.cols = "chas")

r = resample("regr.rpart", bh.task, rdesc, show.info = TRUE)
```

### Blocking

```{r}
# 5 blocks containing 30 observations each
task = makeClassifTask(data = iris, target = "Species", blocking = factor(rep(1:5, each = 30)))
task
```

## Feature Selection

### Filter methods

```{r}
fv = generateFilterValuesData(iris.task, method = "FSelectorRcpp_information.gain")
fv

plotFilterValues(fv, filter = "FSelectorRcpp_information.gain") +
  ggpubr::theme_pubr()

# Keep the 2 most important features
filtered.task = filterFeatures(iris.task, method = "FSelectorRcpp_information.gain", abs = 2)

# Keep the 25% most important features
filtered.task = filterFeatures(iris.task, fval = fv, perc = 0.25)

# Keep all features with importance greater than 0.5
filtered.task = filterFeatures(iris.task, fval = fv, threshold = 0.5)
```

```{r}
lrn = makeFilterWrapper(learner = "classif.fnn",
                        fw.method = "FSelectorRcpp_information.gain", 
                        fw.abs = 2)

rdesc = makeResampleDesc("CV", iters = 10)

r = resample(learner = lrn, 
             task = iris.task, 
             resampling = rdesc, 
             show.info = TRUE, 
             models = TRUE)
r$aggr
r$models
sfeats = sapply(r$models, getFilteredFeatures)
table(sfeats)
```
```{r}
lrn = makeFilterWrapper(learner = "regr.ksvm", fw.method = "FSelector_chi.squared")

ps = makeParamSet(makeNumericParam("fw.perc", lower = 0, upper = 1),
                  makeNumericParam("C", lower = -10, upper = 10, trafo = function(x) 2^x),
                  makeNumericParam("sigma", lower = -10, upper = 10, trafo = function(x) 2^x))

rdesc = makeResampleDesc("CV", iters = 3)

# tuneParams() performs the cross-validation for every element of the cross-product
# and selects the parameter setting with the best mean performance.
res = tuneParams(lrn, 
                 task = bh.task, 
                 resampling = rdesc, 
                 par.set = ps,
                 control = makeTuneControlRandom(maxit = 5))

as.data.frame(res$opt.path)

res$x
res$y

lrn = makeFilterWrapper(learner = "regr.lm", 
                        fw.method = "FSelector_chi.squared",
                        fw.perc = res$x$fw.perc, 
                        C = res$x$C, 
                        sigma = res$x$sigma)
mod = train(lrn, bh.task)
mod
getFilteredFeatures(mod)
```



### Wrapper methods

```{r}
# Specify the search strategy
ctrl = makeFeatSelControlRandom(maxit = 20L)
ctrl

# Resample description
rdesc = makeResampleDesc("Holdout")

# Select features
sfeats = selectFeatures(learner = "surv.coxph", task = wpbc.task, resampling = rdesc,
  control = ctrl, show.info = FALSE)
sfeats
```

```{r}
task = makeClassifTask(data = iris, target = "Species")
lrn = makeLearner("classif.ranger", importance = c("permutation"))
mod = train(lrn, task)

getFeatureImportance(mod)
```


## Nested Resampling

```{r}
# Tuning in inner resampling loop
ps = makeParamSet(
  makeDiscreteParam("C", values = 2^(-2:2)),
  makeDiscreteParam("sigma", values = 2^(-2:2))
)
ctrl = makeTuneControlGrid()
inner = makeResampleDesc("Subsample", iters = 2)
lrn = makeTuneWrapper("classif.ksvm", 
                      resampling = inner, 
                      par.set = ps, 
                      control = ctrl, 
                      show.info = TRUE)

# Outer resampling loop
outer = makeResampleDesc("CV", iters = 3)
r = resample(lrn, 
             iris.task, 
             resampling = outer, 
             extract = getTuneResult, 
             show.info = TRUE)

r
r$extract
opt.paths = getNestedTuneResultsOptPathDf(r)
head(opt.paths, 10)

ggplot(opt.paths, aes(x = C, y = sigma, fill = mmce.test.mean)) + 
  geom_tile() + 
  facet_wrap(~ iter)

getNestedTuneResultsX(r)
getResamplingIndices(r, inner = TRUE)
```

## Performance

```{r}
# Performance measures for classification with multiple classes
listMeasures("classif", properties = "classif.multi")

# Performance measure suitable for the iris classification task
listMeasures(iris.task)
```

```{r}
n = getTaskSize(bh.task)
lrn = makeLearner("regr.gbm", n.trees = 1000)
mod = train(lrn, task = bh.task, subset = seq(1, n, 2))
pred = predict(mod, task = bh.task, subset = seq(2, n, 2))

performance(pred)
```

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, task = sonar.task)
pred = predict(mod, task = sonar.task)

performance(pred, measures = auc)
```

```{r}
lrn = makeLearner("classif.lda", predict.type = "prob")
n = getTaskSize(sonar.task)
mod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2))
pred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2))

# Performance for the default threshold 0.5
performance(pred, measures = list(fpr, fnr, mmce))

# Plot false negative and positive rates as well as the error rate versus the threshold
d = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))

plotThreshVsPerf(d)
```



## pdp

```{r}
# Load required packages
library(pdp)  # for constructing PDPs
library(randomForest)  # for random forest algorithm

# Load the Boston housing data
data (boston)  # included with the pdp package

# Fit a random forest to the boston housing data
set.seed(101)  # for reproducibility
boston.rf <- randomForest(cmedv ~ ., data = boston)

# Partial dependence of lstat and rm on cmedv
grid.arrange(
  partial(boston.rf, pred.var = "lstat", plot = TRUE, rug = TRUE),
  partial(boston.rf, pred.var = "rm", plot = TRUE, rug = TRUE),
  partial(boston.rf, pred.var = c("lstat", "rm"), plot = TRUE, chull = TRUE),
  ncol = 3
)
```


```{r}
# Load required packages
library(kernlab)  # for fitting SVMs

# Fit an SVM to the Pima Indians diabetes data
data (pima)  # load the boston housing data
pima.svm <- ksvm(diabetes ~ ., 
                 data = pima, 
                 type = "C-svc", 
                 kernel = "rbfdot",
                 C = 0.5, 
                 prob.model = TRUE)

# Partial dependence of glucose and age on diabetes test result (neg/pos).
grid.arrange(
  partial(pima.svm, pred.var = "glucose", plot = TRUE, train = pima),
  partial(pima.svm, pred.var = "age", plot = TRUE, train = pima),
  partial(pima.svm, pred.var = c("glucose", "age"), plot = TRUE, train = pima),
  ncol = 3
)
```

```{r}
# Load required packages
library(caret)  # for model training/tuning

# Set up for 5-fold cross-validation
ctrl <- trainControl(method = "cv", number = 5)

# Grid of tuning parameter values
xgb.grid <- expand.grid(
  nrounds = c(1000, 2000),
  max_depth = 2:4,
  eta = c(0.001, 0.01, 0.1),
  gamma = 0, 
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = c(0.5, 0.75, 1)
)

# Tune am XGBoost model to the Pima Indians diabetes data. This may take a few 
# minutes!
set.seed(103)  # for reproducibility
pima.xgb <- train(diabetes ~ ., 
                  data = pima, 
                  method = "xgbTree",
                  prob.model = TRUE, 
                  na.action = na.omit, 
                  trControl = ctrl,
                  tuneGrid = xgb.grid)

# Partial dependence of glucose and age on diabetes test result (neg/pos)
grid.arrange(
  partial(pima.xgb, pred.var = "glucose", plot = TRUE, rug = TRUE),
  partial(pima.xgb, pred.var = "age", plot = TRUE, rug = TRUE),
  partial(pima.xgb, pred.var = "mass", plot = TRUE, rug = TRUE),
  ncol = 3 
)


```


```{r}
lrn.classif = makeLearner("classif.ksvm", predict.type = "prob")
fit.classif = train(lrn.classif, iris.task)
pd = generatePartialDependenceData(fit.classif, iris.task, "Petal.Width")
## Loading required package: mmpf
pd
```

```{r}
# Load required packages
library(randomForest)  # for fitting random forests
library(pdp)           # for partial dependence plots
library(vip)           # for variable importance plots

# Fit a random forest to the Boston housing data
set.seed(101)  # for reproducibility
boston_rf <- randomForest(cmedv ~ ., data = boston, importance = TRUE)

# Variable importance plot (compare to randomForest::varImpPlot(boston_rf))
vip(boston_rf, bar = FALSE, horizontal = FALSE, size = 1.5) # Figure 1


partialPlot(boston_rf, pred.data = boston, x.var = "lstat") 
```

```{r}
# Load required packages
library(ggplot2)
library(pdp)

# Default lattice-based PDP
p1 <- partial(boston_rf, pred.var = "lstat", plot = TRUE, rug = TRUE)

# Switch to ggplot2
p2 <- partial(boston_rf, pred.var = "lstat", plot = TRUE,
              plot.engine = "ggplot2")

# Figure 3
grid.arrange(p1, p2, ncol = 2)
```

```{r}
# lattice-based PDP
p1 <- boston_rf %>%  # the %>% operator is read as "and then"
  partial(pred.var = "lstat") %>%
  plotPartial(smooth = TRUE, lwd = 2, ylab = expression(f(lstat)),
              main = "lattice-based PDP")

# ggplot2-based PDP
p2 <- boston_rf %>%  # the %>% operator is read as "and then"
  partial(pred.var = "lstat") %>%
  autoplot(smooth = TRUE, ylab = expression(f(lstat))) +
  theme_light() +
  ggtitle("ggplot2-based PDP")

# Figure 4
grid.arrange(p1, p2, ncol = 2)  
```

```{r}
library(kernlab)
library(pdp)
library(vip)

# Fit an SVM to the Edgar Anderson's iris data
iris_svm <- ksvm(Species ~ ., 
                 data = iris, 
                 kernel = "rbfdot", 
                 kpar = list(sigma = 0.709), 
                 C = 0.5, 
                 prob.model = TRUE)

# Variable importance plot
vip(object = iris_svm, 
    feature_names = colnames(iris_svm@xmatrix[[1]]), 
    train = iris)

# Construct PDPs for each feature and stores results in a list
features <- names(subset(iris, select = -Species))
pdps <- list()
for (feature in features) {
  pdps[[feature]] <- partial(iris_svm, 
                             pred.var = feature, 
                             ice = TRUE, 
                             plot = TRUE, 
                             rug = TRUE, 
                             train = iris, 
                             alpha = 0.1)
}
grid.arrange(grobs = pdps, ncol = 2)  # display plots in a grid

partial(iris_svm, 
        pred.var = c("Petal.Width", "Sepal.Width"), 
        plot = TRUE, 
        chull = TRUE, 
        train = iris, 
        prob = TRUE, 
        plot.engine = "ggplot2",
        palette = "magma")

```

```{r}
# Compute partial dependence for each of the three classes
pd <- NULL
for (i in 1:3) {
  tmp <- partial(iris_svm, 
                 pred.var = c("Petal.Width", "Petal.Length"),
                 which.class = i, 
                 grid.resolution = 10, 
                 train = iris)
  pd <- rbind(pd, cbind(tmp, Species = levels(iris$Species)[i]))
}

# Figure 3
library(ggplot2)
ggplot(pd, aes(x = Petal.Width, y = Petal.Length, z = yhat, fill = yhat)) +
  geom_tile() +
  geom_contour(color = "white", alpha = 0.5) +
  scale_fill_distiller(name = "Class-centered\nlogit", palette = "Spectral") +
  theme_bw() +
  facet_grid(~ Species)
```


```{r}
# Load required packages
library(xgboost)  # for gradient boosted decision trees
library(pdp)      # for partial dependence plots (PDPs)
library(vip)      # for variable importance plots (VIPs)

# Load Ames housing data
ames <- AmesHousing::make_ames()


# Find the optimal number of rounds using 5-fold CV
set.seed(749)  # for reproducibility
ames_xgb_cv <- xgboost::xgb.cv(
  data = data.matrix(subset(ames, select = -Sale_Price)),
  label = ames$Sale_Price, 
  objective = "reg:linear", 
  verbose = 0,
  nrounds = 1000, 
  max_depth = 5, 
  eta = 0.1, 
  gamma = 0, 
  nfold = 5,
  early_stopping_rounds = 30
)
print(ames_xgb_cv$best_iteration)  # optimal number of trees

# Fit an XGBoost model to the Boston housing data
set.seed(804)  # for reproducibility
ames_xgb <- xgboost::xgboost(
  data = data.matrix(subset(ames, select = -Sale_Price)),
  label = ames$Sale_Price, 
  objective = "reg:linear", 
  verbose = 0,
  nrounds = ames_xgb_cv$best_iteration, 
  max_depth = 5, 
  eta = 0.1, 
  gamma = 0
)

# Variable importance plot
vip(ames_xgb, num_features = 10)  # 10 is the default

# c-ICE curves and PDPs for Overall_Qual and Gr_Liv_Area
x <- data.matrix(subset(ames, select = -Sale_Price))  # training features
p1 <- partial(ames_xgb, 
              pred.var = "Overall_Qual", 
              ice = TRUE, 
              center = TRUE, 
              plot = TRUE, 
              rug = TRUE, 
              alpha = 0.1, 
              plot.engine = "ggplot2", 
              train = x)
p2 <- partial(ames_xgb, 
              pred.var = "Gr_Liv_Area", 
              ice = TRUE, 
              center = TRUE, 
              plot = TRUE, 
              rug = TRUE, 
              alpha = 0.1, 
              plot.engine = "ggplot2",
              train = x)
p3 <- partial(ames_xgb, 
              pred.var = c("Overall_Qual", "Gr_Liv_Area"),
              plot = TRUE, 
              chull = TRUE, 
              plot.engine = "ggplot2", 
              train = x)

# Figure 2
grid.arrange(p1, p2, p3, ncol = 3)
```



## iml

```{r}
library(mlr3)
task = tsk("iris")
learner = lrn("classif.rpart")

# train a model of this learner for a subset of the task
learner$train(task, row_ids = 1:120)
# this is what the decision tree looks like
learner$model

prediction_iml = Predictor$new(model, data = df_classif, class = "elevated")

predictions = learner$predict(task, row_ids = 121:150)
predictions

# accuracy of our model on the test set of the final 30 rows
predictions$score(msr("classif.acc"))

```


```{r}
data("Boston", package  = "MASS")
Boston

# create an mlr task and model
tsk = makeRegrTask(data = Boston, target = "medv")
lrn = makeLearner("regr.randomForest", ntree = 100)
mod = train(lrn, tsk)

library("iml")
X = Boston[which(names(Boston) != "medv")]
predictor = Predictor$new(mod, data = X, y = Boston$medv)

imp = FeatureImp$new(predictor, loss = "mae")
plot(imp)
imp$results

pdp.obj = Partial$new(predictor, feature = "lstat")
plot(pdp.obj)

pdp.obj$set.feature("rm")
pdp.obj$center(min(Boston$rm))
plot(pdp.obj)

tree = TreeSurrogate$new(predictor, maxdepth = 2)
plot(tree)
```

## Practice

```{r}
train <- read_csv("../../data/train_ctrUa4K.csv")
test <- read_csv("../../data/test_lAUu6dG.csv")
```

```{r}
summarizeColumns(train)
summarizeColumns(test)

hist(train$ApplicantIncome, breaks = 300)
hist(train$CoapplicantIncome, breaks = 100)
boxplot(train$ApplicantIncome)
boxplot(train$CoapplicantIncome)
boxplot(train$LoanAmount)

train$Credit_History <- as.factor(train$Credit_History)
test$Credit_History <- as.factor(test$Credit_History)

train$Dependents <- as.factor(train$Dependents)
levels(train$Dependents)[4] <- "3"
test$Dependents <- as.factor(test$Dependents)
levels(test$Dependents)[4] <- "3"
```


```{r}
imp <- impute(train, 
              classes = list(factor = imputeMode(), integer = imputeMean()), 
              dummy.classes = c("integer","factor"), 
              dummy.type = "numeric")

imp1 <- impute(test, 
               classes = list(factor = imputeMode(), integer = imputeMean()), 
               dummy.classes = c("integer","factor"), 
               dummy.type = "numeric")

imp_train <- imp$data
imp_test <- imp1$data

summarizeColumns(imp_train)
summarizeColumns(imp_test)

rpart_imp <- impute(train, 
                    target = "Loan_Status",
                    classes = list(numeric = imputeLearner(makeLearner("regr.rpart")),
                                   factor = imputeLearner(makeLearner("classif.rpart"))),
                    dummy.classes = c("numeric","factor"),
                    dummy.type = "numeric")

#for train data set
cd <- capLargeValues(imp_train, 
                     target = "Loan_Status",
                     cols = c("ApplicantIncome"),
                     threshold = 40000)
cd <- capLargeValues(cd, 
                     target = "Loan_Status",
                     cols = c("CoapplicantIncome"),
                     threshold = 21000)
cd <- capLargeValues(cd, 
                     target = "Loan_Status",
                     cols = c("LoanAmount"),
                     threshold = 520)

#rename the train data as cd_train
cd_train <- cd 

#add a dummy Loan_Status column in test data
imp_test$Loan_Status <- sample(0:1, size = 367, replace = T)

cde <- capLargeValues(imp_test, 
                      target = "Loan_Status",
                      cols = c("ApplicantIncome"),
                      threshold = 33000)
cde <- capLargeValues(cde, 
                      target = "Loan_Status",
                      cols = c("CoapplicantIncome"),
                      threshold = 16000)
cde <- capLargeValues(cde, 
                      target = "Loan_Status",
                      cols = c("LoanAmount"),
                      threshold = 470)

#renaming test data
cd_test <- cde

#convert numeric to factor - train
for (f in names(cd_train[, c(14:15)])) {
  if( class(cd_train[, c(14:15)] [[f]]) == "numeric"){
    levels <- unique(cd_train[, c(14:15)][[f]])
    cd_train[, c(14:15)][[f]] <- as.factor(factor(cd_train[, c(14:15)][[f]], 
                                                  levels = levels))
  }
}

# convert numeric to factor - test
cd_test <- cd_test %>% 
  mutate(Dependents.dummy = as_factor(Dependents.dummy),
         Credit_History.dummy = as_factor(Credit_History.dummy))

# Total_Income
cd_train$Total_Income <- cd_train$ApplicantIncome + cd_train$CoapplicantIncome
cd_test$Total_Income <- cd_test$ApplicantIncome + cd_test$CoapplicantIncome

# Income by loan
cd_train$Income_by_loan <- cd_train$Total_Income/cd_train$LoanAmount
cd_test$Income_by_loan <- cd_test$Total_Income/cd_test$LoanAmount

#change variable class
cd_train$Loan_Amount_Term <- as.numeric(cd_train$Loan_Amount_Term)
cd_test$Loan_Amount_Term <- as.numeric(cd_test$Loan_Amount_Term)

#Loan amount by term
cd_train$Loan_amount_by_term <- cd_train$LoanAmount/cd_train$Loan_Amount_Term
cd_test$Loan_amount_by_term <- cd_test$LoanAmount/cd_test$Loan_Amount_Term

cd_train$Total_Income <- NULL
cd_test$Total_Income <- NULL

summarizeColumns(cd_train)
summarizeColumns(cd_test)

cd_train <- cd_train %>% 
  select(- Loan_ID) %>% 
  mutate(Gender = as_factor(Gender),
         Married = as_factor(Married),
         Education = as_factor(Education),
         Self_Employed = as_factor(Self_Employed),
         Property_Area = as_factor(Property_Area),
         Loan_Status = as_factor(Loan_Status))

cd_test <- cd_test %>% 
  select(- Loan_ID) %>% 
  mutate(Gender = as_factor(Gender),
         Married = as_factor(Married),
         Education = as_factor(Education),
         Self_Employed = as_factor(Self_Employed),
         Property_Area = as_factor(Property_Area),
         Loan_Status = as_factor(Loan_Status))

#create a task
trainTask <- makeClassifTask(data = cd_train,target = "Loan_Status", positive = "Y")
testTask <- makeClassifTask(data = cd_test, target = "Loan_Status")

getTaskData(trainTask)

#normalize the variables
trainTask <- normalizeFeatures(trainTask, method = "standardize")
testTask <- normalizeFeatures(testTask,method = "standardize")

#Feature importance
im_feat <- generateFilterValuesData(trainTask, 
                                    method = c("FSelector_information.gain"))
plotFilterValues(im_feat,n.show = 20)



#load qda 
qda.learner <- makeLearner("classif.qda", predict.type = "response")

#train model
qmodel <- mlr::train(qda.learner, trainTask)

#predict on test data
> qpredict <- predict(qmodel, testTask)

#create submission file
> submit <- data.frame(Loan_ID = test$Loan_ID, Loan_Status = qpredict$data$response)
> write.csv(submit, "submit1.csv",row.names = F)
```


```{r}
library(earth) # for the ozone1 data
data(ozone1)
oz <- ozone1[, c("O3", "humidity", "temp")] # simple dataset for illustration

library(randomForest)
mod <- randomForest(O3 ~ ., data=oz)

library(plotmo)
plotmo(mod)
```

