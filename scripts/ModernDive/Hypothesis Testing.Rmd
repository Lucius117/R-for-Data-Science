---
title: "Hypothesis Testing"
author: "Xiaochi"
date: "10/01/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(infer)
library(moderndive)
library(nycflights13)
library(ggplot2movies)
```

## Promotions activity

### Does gender affect promotions at a bank?

```{r}
promotions
```

```{r}
ggplot(promotions, aes(x = gender, fill = decision)) +
  geom_bar() +
  labs(x = "Gender of name on résumé")
```

Observe in Figure 9.1 that it appears that résumés with female names were much less likely to be accepted for promotion.

```{r}
promotions %>% 
  group_by(gender, decision) %>% 
  summarize(n = n())
```

So of the 24 résumés with male names, 21 were selected for promotion, for a proportion of 21/24 = 0.875 = 87.5%. 
On the other hand, of the 24 résumés with female names, 14 were selected for promotion, for a proportion of 14/24 = 0.583 = 58.3%.
Comparing these two rates of promotion, it appears that résumés with male names were selected for promotion at a rate 0.875 - 0.583 = 0.292 = 29.2% higher than résumés with female names.
This is suggestive of an advantage for résumés with a male name on it.

The question is, however, does this provide conclusive evidence that there is gender discrimination in promotions at banks?
Could a difference in promotion rates of 29.2% still occur by chance, even in a hypothetical world where no gender-based discrimination existed?
In other words, what is the role of sampling variation in this hypothesized world?

### Shuffling once

First, try to imagine a hypothetical universe where no gender discrimination in promotions existed.
In such a hypothetical universe, the gender of an applicant would have no bearing on their chances of promotion.
Bringing things back to our promotions data frame, the gender variable would thus be an irrelevant label.
If these gender labels were irrelevant, then we could randomly reassign them by “shuffling” them to no consequence!

In our hypothesized universe of no gender discrimination, gender is irrelevant and thus it is of no consequence to randomly “shuffle” the values of gender. 
The number of male and female names remains the same, but they are now listed in a different order.

```{r}
promotions
promotions_shuffled
```

```{r}
ggplot(promotions_shuffled, aes(x = gender, fill = decision)) +
  geom_bar() + 
  labs(x = "Gender of résumé name")
```

It appears the difference in “male names” versus “female names” promotion rates is now different. 
Compared to the original data, the new “shuffled” has promotion rates that are much more similar.

```{r}
promotions_shuffled %>% 
  group_by(gender, decision) %>% 
  summarize(n = n())
```

So in this hypothetical universe of no discrimination, $18/24=0.75=75\%$ of male resumes were selected for promotion.
On the other hand, $17/24=0.708=70.8\%$ of female resumes were selected for promotion.

### Shuffling 16 times

### What did we just do?

hypothesis testing using a permutation test.
The term “permutation” is the mathematical term for “shuffling”: taking a series of values and reordering them randomly, as you did with the playing cards.

In fact, permutations are another form of resampling, like the bootstrap method.
While the bootstrap method involves resampling with replacement, permutation methods involve resampling without replacement.

After sampling a penny, you put it back in the hat. 
After drawing a card, you laid it out in front of you, recorded the color, and then you did not put it back in the deck.

In our previous example, we tested the validity of the hypothesized universe of no gender discrimination. 
The evidence contained in our observed sample of 48 résumés was somewhat inconsistent with our hypothesized universe. 
Thus, we would be inclined to reject this hypothesized universe and declare that the evidence suggests there is gender discrimination.

## Understanding hypothesis tests

First, a __hypothesis__ is a statement about the value of an unknown population parameter.

Second, a __hypothesis test__ consists of a test between two competing hypotheses: (1) a null hypothesis $H_0$ (pronounced “H-naught”) versus (2) an alternative hypothesis $H_A$ (also denoted $H_1$).

Third, a __test statistic__ is a _point estimate/sample statistic_ formula used for hypothesis testing.

Fourth, the __observed test statistic__ is the value of the __test statistic__ that we observed in real life.

Fifth, the null distribution is the sampling distribution of the test statistic assuming the null hypothesis $H_0$ is true.

Sixth, the __p-value__ is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis $H_0$ is true.
You can think of the p-value as a quantification of “surprise”: assuming
$H_0$ is true, how surprised are we with what we observed?
In our hypothesized universe of no gender discrimination, how surprised are we that we observed a difference in promotion rates of 0.292.
The p-value quantifies this probability.
In the case of our 16 differences in sample proportions, what proportion had a more “extreme” result?
Here, extreme is defined in terms of the alternative hypothesis $H_A$ that “male” applicants are promoted at a higher rate than “female” applicants.
In other words, how often was the discrimination in favor of men even more pronounced than $0.875-0.583=0.292=29.2\%$?
In this case, 0 times out of 16, we obtained a difference in proportion greater than or equal to the observed difference of 0.292 = 29.2%.
A very rare (in fact, not occurring) outcome!
Given the rarity of such a pronounced difference in promotion rates in our hypothesized universe of no gender discrimination, we’re inclined to reject our hypothesized universe.
Instead, we favor the hypothesis stating there is discrimination in favor of the “male” applicants.
In other words, we reject $H_0$ in favor of $H_A$.

Seventh and lastly, in many hypothesis testing procedures, it is commonly recommended to set the __significance level__ of the test beforehand.
This value acts as a cutoff on the p-value, where if the p-value falls below $\alpha$, we would “reject the null hypothesis $H_0$."
Alternatively, if the p-value does not fall below $\alpha$, we would “fail to reject $H_0$.” 
Note the latter statement is not quite the same as saying we “accept $H_0$.”

## Conducting hypothesis tests

### infer package workflow

```{r}
null_distribution <- promotions %>% 
  specify(formula = decision ~ gender, success = "promoted") %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "diff in props", order = c("male", "female"))
null_distribution
```

Observe that we have 1000 values of `stat`, each representing one instance of $\hat{p}_m-\hat{p}_f$ in a hypothesized world of no gender discrimination.
Observe as well that we chose the name of this data frame carefully: `null_distribution`.
The sampling distributions when the null hypothesis $H_0$ is assumed to be true have a special name: the null distribution.

```{r}
obs_diff_prop <- promotions %>% 
  specify(decision ~ gender, success = "promoted") %>% 
  calculate(stat = "diff in props", order = c("male", "female"))
obs_diff_prop
```

```{r}
visualize(null_distribution, bins = 10) +
  shade_p_value(obs_stat = obs_diff_prop, direction = "right")
```

A p-value is the probability of obtaining a test statistic just as, or more extreme than, the observed test statistic, assuming the null hypothesis $H_0$ is true.

So judging by the shaded region in Figure 9.11, it seems we would somewhat rarely observe differences in promotion rates of 0.292 = 29.2% or more in a hypothesized universe of no gender discrimination.
In other words, the p-value is somewhat small.
Hence, we would be inclined to reject this hypothesized universe, or using statistical language we would “reject $H_0$.”

What fraction of the null distribution is shaded?
In other words, what is the exact value of the p-value?

```{r}
null_distribution %>% 
  get_p_value(obs_stat = obs_diff_prop, direction = "right")
```

### Comparison with confidence intervals

create the null distribution to compute the p-value

```{r}
null_distribution <- promotions %>% 
  specify(formula = decision ~ gender, success = "promoted") %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "diff in props", order = c("male", "female"))
```

create the bootstrap distribution to construct a 95% confidence interval

```{r}
bootstrap_distribution <- promotions %>% 
  specify(formula = decision ~ gender, success = "promoted") %>% 
  # Change 1 - Remove hypothesize():
  # hypothesize(null = "independence") %>% 
  # Change 2 - Switch type from "permute" to "bootstrap":
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "diff in props", order = c("male", "female"))
```

```{r}
percentile_ci <- bootstrap_distribution %>% 
  get_confidence_interval(level = 0.95, type = "percentile")
percentile_ci
```

we are 95% “confident” that the true difference in population proportions $p_m-p_f$ is between (0.049, 0.53). 

```{r}
visualize(bootstrap_distribution) + 
  shade_confidence_interval(endpoints = percentile_ci)
```

Notice a key value that is not included in the 95% confidence interval for $p_m-p_f$: the value 0.
In other words, a difference of 0 is not included in our net, suggesting that $p_m$ and $p_f$ are truly different!
Furthermore, observe how the entirety of the 95% confidence interval for  
$p_m-p_f$ lies above 0, suggesting that this difference is in favor of men.

```{r}
se_ci <- bootstrap_distribution %>% 
  get_confidence_interval(level = 0.95, type = "se", 
                          point_estimate = obs_diff_prop)
se_ci
```

```{r}
visualize(bootstrap_distribution) + 
  shade_confidence_interval(endpoints = se_ci)
```

Again, notice how the value 0 is not included in our confidence interval, again suggesting that $p_m$ and $p_f$ are truly different!

### “There is only one test”

The steps to conduct a hypothesis test:

1. specify() the variables of interest in your data frame.
2. hypothesize() the null hypothesis $H_0$. 
In other words, set a “model for the universe” assuming $H_0$ is true.
3. generate() shuffles assuming $H_0$ is true. 
In other words, simulate data assuming $H_0$ is true.
4. calculate() the test statistic of interest, both for the observed data and your simulated data.
5. visualize() the resulting null distribution and compute the p-value by comparing the null distribution to the observed test statistic.

## Interpreting hypothesis tests

### Two possible outcomes

### Types of errors

### How do we choose alpha?

## Case study: Are action or romance movies rated higher?

### IMDb ratings data

```{r}
movies
movies_sample
```

```{r}
ggplot(data = movies_sample, aes(x = genre, y = rating)) +
  geom_boxplot() +
  labs(y = "IMDb rating")
```

Eyeballing Figure 9.17, romance movies have a higher median rating. 
Do we have reason to believe, however, that there is a significant difference between the mean rating for action movies compared to romance movies?
It’s hard to say just based on this plot. 
The boxplot does show that the median sample rating is higher for romance movies.

```{r}
movies_sample %>% 
  group_by(genre) %>% 
  summarize(n = n(), mean_rating = mean(rating), std_dev = sd(rating))
```

Observe that we have 36 romance movies with an average rating of 6.322 stars and 32 action movies with an average rating of 5.275 stars. 
The difference in these average ratings is thus 6.322 - 5.275 = 1.047.
So there appears to be an edge of 1.047 stars in favor of romance movies. 
The question is, however, are these results indicative of a true difference for all romance and action movies? 
Or could we attribute this difference to chance sampling variation?

### Sampling scenario

### Conducting the hypothesis test

The null hypothesis $H_0$ suggests that both romance and action movies have the same mean rating.
This is the “hypothesized universe” we’ll assume is true.

The alternative hypothesis $H_a$ suggests that there is a difference.

```{r}
null_distribution_movies <- movies_sample %>% 
  specify(formula = rating ~ genre) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "diff in means", order = c("Action", "Romance"))
null_distribution_movies
```

```{r}
obs_diff_means <- movies_sample %>% 
  specify(formula = rating ~ genre) %>% 
  calculate(stat = "diff in means", order = c("Action", "Romance"))
obs_diff_means
```

```{r}
visualize(null_distribution_movies, bins = 10) + 
  shade_p_value(obs_stat = obs_diff_means, direction = "both")
```

First, the histogram is the null distribution.
Second, the solid line is the observed test statistic, or the difference in sample means we observed in real life of $5.275-6.322=-1.047$.
Third, the two shaded areas of the histogram form the p-value, or the probability of obtaining a test statistic just as or more extreme than the observed test statistic assuming the null hypothesis $H_0$ is true.

What proportion of the null distribution is shaded?
what is the numerical value of the p-value? 

```{r}
null_distribution_movies %>% 
  get_p_value(obs_stat = obs_diff_means, direction = "both")
```

This p-value of 0.004 is very small. 
In other words, there is a very small chance that we’d observe a difference of 5.275 - 6.322 = -1.047 in a hypothesized universe where there was truly no difference in ratings.
But this p-value is larger than our (even smaller) pre-specified  
α significance level of 0.001.
Thus, we are inclined to fail to reject the null hypothesis $H_0:\mu_a-\mu_r=0$.
We do not have the evidence needed in this sample of data to suggest that we should reject the hypothesis that there is no difference in mean IMDb ratings between romance and action movies.
We, thus, cannot say that a difference exists in romance and action movie ratings, on average, for all IMDb movies.

## Conclusion

### Theory-based hypothesis tests

```{r}
movies_sample %>% 
  group_by(genre) %>% 
  summarize(n = n(), mean_rating = mean(rating), std_dev = sd(rating))
```

```{r}
# Construct null distribution of xbar_a - xbar_m:
null_distribution_movies <- movies_sample %>% 
  specify(formula = rating ~ genre) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "diff in means", order = c("Action", "Romance"))
visualize(null_distribution_movies, bins = 10)
```

```{r}
# Construct null distribution of t:
null_distribution_movies_t <- movies_sample %>% 
  specify(formula = rating ~ genre) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  # Notice we switched stat from "diff in means" to "t"
  calculate(stat = "t", order = c("Action", "Romance"))
visualize(null_distribution_movies_t, bins = 10)
```

Observe that while the shape of the null distributions of both the difference in means $\bar{x}_a-\bar{x}_r$ and the two-sample t-statistics are similar, the scales on the x-axis are different.
The two-sample t-statistic values are spread out over a larger range.

```{r}
visualize(null_distribution_movies_t, bins = 10, method = "both")
```

Observe that the curve does a good job of approximating the histogram here.
To calculate the p-value in this case, we need to figure out how much of the total area under the t-distribution curve is at or “more extreme” than our observed two-sample t-statistic.
Since $H_A:\mu_a-\mu_r\neq0$ is a two-sided alternative, we need to add up the areas in both tails.

```{r}
obs_two_sample_t <- movies_sample %>% 
  specify(formula = rating ~ genre) %>% 
  calculate(stat = "t", order = c("Action", "Romance"))
obs_two_sample_t
```

```{r}
visualize(null_distribution_movies_t, method = "both") +
  shade_p_value(obs_stat = obs_two_sample_t, direction = "both")
```

```{r}
null_distribution_movies_t %>% 
  get_p_value(obs_stat = obs_two_sample_t, direction = "both")
```

We have a very small p-value, and thus it is very unlikely that these results are due to sampling variation. Thus, we are inclined to reject $H_0$.

To be able to use the t-test and other such theoretical methods, there are always a few conditions to check:

1. Nearly normal populations or large sample sizes. 
A general rule of thumb that works in many (but not all) situations is that the sample size n should be greater than 30.
2. Both samples are selected independently of each other.
3. All observations are independent from each other.

If any of the conditions were clearly not met, we couldn’t put as much trust into any conclusions reached.
On the other hand, in most scenarios, the only assumption that needs to be met in the simulation-based method is that the sample is selected at random.

### When inference is not needed

Of all flights leaving a New York City airport, are Hawaiian Airlines flights in the air for longer than Alaska Airlines flights?

```{r}
flights_sample <- flights %>% 
  filter(carrier %in% c("HA", "AS"))
flights_sample
```

There are two possible statistical inference methods we could use to answer such questions:

1. we could construct a 95% confidence interval for the difference in population means $\mu_{HA}-\mu_{AS}$, where $\mu_{HA}$ is the mean air time of all Hawaiian Airlines flights and $\mu_{AS}$ is the mean air time of all Alaska Airlines flights.
We could then check if the entirety of the interval is greater than 0, suggesting that $\mu_{HA}-\mu_{AS}>0$, or, in other words suggesting that $\mu_{HA}>\mu_{AS}$.

2. we could perform a hypothesis test of the null hypothesis $H_0:\mu_{HA}-\mu_{AS}=0$ versus the alternative hypothesis $H_A:\mu_{HA}-\mu_{AS}>0$.

```{r}
ggplot(data = flights_sample, mapping = aes(x = carrier, y = air_time)) +
  geom_boxplot() +
  labs(x = "Carrier", y = "Air Time")
```

```{r}
flights_sample %>% 
  group_by(carrier, dest) %>% 
  summarize(n = n(), mean_time = mean(air_time, na.rm = TRUE))
```



```{r}
library(nycflights13)
library(dplyr)
library(stringr)
library(infer)
set.seed(2017)
fli_small <- flights %>% 
  sample_n(size = 500) %>% 
  mutate(half_year = case_when(
    between(month, 1, 6) ~ "h1",
    between(month, 7, 12) ~ "h2"
  )) %>% 
  mutate(day_hour = case_when(
    between(hour, 1, 12) ~ "morning",
    between(hour, 13, 24) ~ "not morning"
  )) %>% 
  select(arr_delay, dep_delay, half_year, 
         day_hour, origin, carrier)
```

```{r}
fli_small
```

```{r}
obs_t <- fli_small %>%
  specify(arr_delay ~ half_year) %>%
  calculate(stat = "t", order = c("h1", "h2"))
obs_t
```

```{r}
t_null_perm <- fli_small %>%
  # alt: response = arr_delay, explanatory = half_year
  specify(arr_delay ~ half_year) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "t", order = c("h1", "h2"))
t_null_perm
```

```{r}
visualize(t_null_perm) +
  shade_p_value(obs_stat = obs_t, direction = "two_sided")
```

```{r}
t_null_perm %>% 
  get_p_value(obs_stat = obs_t, direction = "two_sided")
```

```{r}
t_null_theor <- fli_small %>%
  # alt: response = arr_delay, explanatory = half_year
  specify(arr_delay ~ half_year) %>%
  hypothesize(null = "independence") %>%
  # generate() ## Not used for theoretical
  calculate(stat = "t", order = c("h1", "h2"))
```

```{r}
visualize(t_null_theor, method = "theoretical") +
  shade_p_value(obs_stat = obs_t, direction = "two_sided")
```

```{r}
visualize(t_null_perm, method = "both") +
  shade_p_value(obs_stat = obs_t, direction = "two_sided")
```

```{r}
fli_small %>% 
  t_test(formula = arr_delay ~ half_year,
         alternative = "two_sided",
         order = c("h1", "h2")) %>% 
  dplyr::pull(p_value)
```

