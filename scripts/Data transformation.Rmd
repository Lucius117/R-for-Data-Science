---
title: "Data transformation"
author: "Xiaochi"
date: "05/08/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = TRUE,
	warning = TRUE
)

library(nycflights13)
library(tidyverse)
```

# Data

```{r}
flights
```

You might notice that this data frame prints a little differently from other data frames you might have used in the past: it only shows the first few rows and all the columns that fit on one screen. (To see the whole dataset, you can run View(flights) which will open the dataset in the RStudio viewer). It prints differently because it’s a tibble. Tibbles are data frames, but slightly tweaked to work better in the tidyverse.

You might also have noticed the row of three (or four) letter abbreviations under the column names. These describe the type of each variable:

* int stands for integers.

* dbl stands for doubles, or real numbers.

* chr stands for character vectors, or strings.

* dttm stands for date-times (a date + a time).

* lgl stands for logical, vectors that contain only TRUE or FALSE.

* fctr stands for factors, which R uses to represent categorical variables with fixed possible values.

* date stands for dates.

In this chapter you are going to learn the five key dplyr functions that allow you to solve the vast majority of your data manipulation challenges:

* Pick observations by their values (filter()).

* Reorder the rows (arrange()).

* Pick variables by their names (select()).

* Create new variables with functions of existing variables (mutate()).

* Collapse many values down to a single summary (summarise()).

These can all be used in conjunction with group_by() which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These six functions provide the verbs for a language of data manipulation.

All verbs work similarly:

* The first argument is a data frame.

* The subsequent arguments describe what to do with the data frame, using the variable names (without quotes).

* The result is a new data frame.

# Filter rows with filter()

```{r}
filter(flights, month == 1, day == 1)
```

When you run that line of code, dplyr executes the filtering operation and returns a new data frame. dplyr functions never modify their inputs, so if you want to save the result, you’ll need to use the assignment operator, <-:

```{r}
jan1 <- filter(flights,month==1,day==1)
```

R either prints out the results, or saves them to a variable. If you want to do both, you can wrap the assignment in parentheses:

```{r}
(dec25 <- filter(flights,month==12,day==25))
```

R provides the standard suite: 
* > 
* >= 
* < 
* <= 
* != (not equal)
* and == (equal).

Multiple arguments to filter() are combined with “and”: every expression must be true in order for a row to be included in the output. For other types of combinations, you’ll need to use Boolean operators yourself: 
* & is “and”
* | is “or”
* ! is “not”

```{r}
filter(flights,month==11|month==12)
```

x %in% y. This will select every row where x is one of the values in y.

```{r}
nov_dec <- filter(flights,month %in% c(11,12))
```


```{r}
filter(flights,!(arr_delay>120|dep_delay>120))
filter(flights,arr_delay<=120,dep_delay<=120)
```

NA represents an unknown value so missing values are “contagious”: almost any operation involving an unknown value will also be unknown.The most confusing result is this one:

```{r}
NA==NA
```

If you want to determine if a value is missing, use is.na():ˆ

filter() only includes rows where the condition is TRUE; it excludes both FALSE and NA values. If you want to preserve missing values, ask for them explicitly:

```{r}
df <- tibble(x=c(1,NA,3))
filter(df,x>1)
filter(df,is.na(x)|x>1)
```

# Arrange rows with arrange()

arrange() works similarly to filter() except that instead of selecting rows, it changes their order. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:

```{r}
flights

arrange(flights,year,month,day)
```

Use desc() to re-order by a column in descending order:

```{r}
arrange(flights, desc(dep_delay))
```

```{r}
df <- tibble(x = c(5, 2, NA))
arrange(df, x)
arrange(df, desc(x))
```

# Select solumns with select()

```{r}
select(flights, year, month, day)
select(flights, year:day)
select(flights, -(year:day))
```

There are a number of helper functions you can use within select():

* starts_with("abc"): matches names that begin with “abc”.

* ends_with("xyz"): matches names that end with “xyz”.

* contains("ijk"): matches names that contain “ijk”.

* matches("(.)\\1"): selects variables that match a regular expression. This one matches any variables that contain repeated characters.

* num_range("x", 1:3): matches x1, x2 and x3.

use rename(), which is a variant of select() that keeps all the variables that aren’t explicitly mentioned:

```{r}
rename(flights,tail_num=tailnum)
```

Another option is to use select() in conjunction with the everything() helper. This is useful if you have a handful of variables you’d like to move to the start of the data frame.

```{r}
select(flights,time_hour,air_time,everything())
```

# Add new variables with mutate()

mutate() always adds new columns at the end of your dataset so we’ll start by creating a narrower dataset so we can see the new variables. Remember that when you’re in RStudio, the easiest way to see all the columns is View().

```{r}
flights_sml <- select(flights,
                      year:day,
                      ends_with("delay"),
                      distance,
                      air_time)

mutate(flights_sml,
       gain=dep_delay-arr_delay,
       speed=distance/air_time*60)

mutate(flights_sml,
       gain=dep_delay-arr_delay,
       hours=air_time/60,
       gain_per_hour=gain/hours)
```

If you only want to keep the new variables, use transmute():

```{r}
transmute(flights,
          gain=dep_delay-arr_delay,
          hours=air_time/60,
          gain_per_hour=gain/hours)
```

```{r}
transmute(flights,
          dep_time,
          hour = dep_time %/% 100,
          minute = dep_time %% 100)
```

```{r}
x <- 1:10

log(x)
log2(x)
log10(x)
```

# Grouped summaries with summarise()

The last key verb is summarise(). It collapses a data frame to a single row:

```{r}
summarise(flights,delay=mean(dep_delay,na.rm=TRUE))
```

summarise() is not terribly useful unless we pair it with group_by(). This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the dplyr verbs on a grouped data frame they’ll be automatically applied “by group”. For example, if we applied exactly the same code to a data frame grouped by date, we get the average delay per date:

```{r}
by_day <- group_by(flights,year,month,day)

summarise(by_day,delay=mean(dep_delay,na.rm=TRUE))
```

Together group_by() and summarise() provide one of the tools that you’ll use most commonly when working with dplyr: grouped summaries. But before we go any further with this, we need to introduce a powerful new idea: the pipe.

## Pipe

Imagine that we want to explore the relationship between the distance and average delay for each location. Using what you know about dplyr, you might write code like this:

```{r}
by_dest <- group_by(flights,dest)
delay <- summarise(by_dest,
                   count=n(),
                   dist=mean(distance,na.rm = TRUE),
                   delay=mean(arr_delay,na.rm=TRUE))
delay <- filter(delay,count>20,dest!="HNL")

ggplot(data=delay,mapping=aes(x=dist,y=delay))+
  geom_point(aes(size=count),alpha=1/3)+
  geom_smooth()

```

There are three steps to prepare this data:

* Group flights by destination.

* Summarise to compute distance, average delay, and number of flights.

* Filter to remove noisy points and Honolulu airport, which is almost twice as far away as the next closest airport.

```{r}
delays <- flights %>% 
  group_by(dest) %>% 
  summarise(
    count=n(),
    dist=mean(distance,na.rm=TRUE),
    delay=mean(arr_delay,na.rm=TRUE)
  ) %>% 
  filter(count>20,dest!="NHL")
```

This focuses on the transformations, not what’s being transformed, which makes the code easier to read. You can read it as a series of imperative statements: group, then summarise, then filter. As suggested by this reading, a good way to pronounce %>% when reading code is “then”

Behind the scenes, x %>% f(y) turns into f(x, y), and x %>% f(y) %>% g(z) turns into g(f(x, y), z) and so on. You can use the pipe to rewrite multiple operations in a way that you can read left-to-right, top-to-bottom. We’ll use piping frequently from now on because it considerably improves the readability of code, and we’ll come back to it in more detail in pipes.

Working with the pipe is one of the key criteria for belonging to the tidyverse. The only exception is ggplot2: it was written before the pipe was discovered. Unfortunately, the next iteration of ggplot2, ggvis, which does use the pipe, isn’t quite ready for prime time yet.

## Missing Values

You may have wondered about the na.rm argument we used above. What happens if we don’t set it?

```{r}
flights %>% 
  group_by(year,month,day) %>% 
  summarise(mean=mean(dep_delay))
```

We get a lot of missing values! That’s because aggregation functions obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. Fortunately, all aggregation functions have an na.rm argument which removes the missing values prior to computation:

```{r}
flights %>% 
  group_by(year,month,day) %>% 
  summarise(mean=mean(dep_delay,na.rm=TRUE))
```

In this case, where missing values represent cancelled flights, we could also tackle the problem by first removing the cancelled flights. We’ll save this dataset so we can reuse it in the next few examples.

```{r}
not_cancelled <- flights %>% 
  filter(!is.na(dep_delay),!is.na(arr_delay))

not_cancelled %>% 
  group_by(year,month,day) %>% 
  summarise(mean=mean(dep_delay))
```

## Counts

Whenever you do any aggregation, it’s always a good idea to include either a count (n()), or a count of non-missing values (sum(!is.na(x))). That way you can check that you’re not drawing conclusions based on very small amounts of data. For example, let’s look at the planes (identified by their tail number) that have the highest average delays:

```{r}
delays <- not_cancelled %>% 
  group_by(tailnum) %>% 
  summarise(
    delay=mean(arr_delay)
  )

ggplot(data=delays,mapping=aes(x=delay))+
  geom_freqpoly(binwidth=10)
```

Wow, there are some planes that have an average delay of 5 hours (300 minutes)!

The story is actually a little more nuanced. We can get more insight if we draw a scatterplot of number of flights vs. average delay:

```{r}
delays <- not_cancelled %>% 
  group_by(tailnum) %>% 
  summarise(
    delay=mean(arr_delay,na.rm=TRUE),
    n=n()
  )

ggplot(data=delays,mapping=aes(x=n,y=delay))+
  geom_point(alpha=1/10)
```

Not surprisingly, there is much greater variation in the average delay when there are few flights. The shape of this plot is very characteristic: whenever you plot a mean (or other summary) vs. group size, you’ll see that the variation decreases as the sample size increases.

```{r}
delays %>% 
  filter(n>30) %>% 
  ggplot(mapping=aes(x=n,y=delay))+
    geom_point(alpha=1/10)
```

```{r}
batting <- as_tibble(Lahman::Batting)

batters <- batting %>% 
  group_by(playerID) %>% 
  summarise(
    ba=sum(H,na.rm=TRUE)/sum(AB,na.rm=TRUE),
    ab=sum(AB,na.rm=TRUE)
  )

batters %>% 
  filter(ab>100) %>% 
  ggplot(mapping=aes(x=ab,y=ba))+
    geom_point()+
    geom_smooth(se=FALSE)

```

## Useful summary functions

Just using means, counts, and sum can get you a long way, but R provides many other useful summary functions:

* Measures of location: we’ve used mean(x), but median(x) is also useful. The mean is the sum divided by the length; the median is a value where 50% of x is above it, and 50% is below it.

* It’s sometimes useful to combine aggregation with logical subsetting. We haven’t talked about this sort of subsetting yet, but you’ll learn more about it in subsetting.

```{r}
not_cancelled %>% 
  group_by(year,month,day) %>% 
  summarise(
    avg_delay1=mean(arr_delay),
    avg_delay2=mean(arr_delay[arr_delay>0])
  )
```

* Measures of spread: sd(x), IQR(x), mad(x). The root mean squared deviation, or standard deviation sd(x), is the standard measure of spread. The interquartile range IQR(x) and median absolute deviation mad(x) are robust equivalents that may be more useful if you have outliers.

```{r}
# Why is distance to some destinations more variable than to others?
not_cancelled %>% 
  group_by(dest) %>% 
  summarise(distance_sd=sd(distance)) %>% 
  arrange(desc(distance_sd))
```

* Measures of rank: min(x), quantile(x, 0.25), max(x). Quantiles are a generalisation of the median. For example, quantile(x, 0.25) will find a value of x that is greater than 25% of the values, and less than the remaining 75%.

```{r}
# When do the first and last flights leave each day?
not_cancelled %>% 
  group_by(year,month,day) %>% 
  summarise(
    first=min(dep_time),
    last=max(dep_time)
  )
```

* Measures of position: first(x), nth(x, 2), last(x). These work similarly to x[1], x[2], and x[length(x)] but let you set a default value if that position does not exist (i.e. you’re trying to get the 3rd element from a group that only has two elements). For example, we can find the first and last departure for each day:

```{r}
not_cancelled %>% 
  group_by(year,month,day) %>% 
  summarise(
    first_dep=first(dep_time),
    last_dep=last(dep_time)
  )
```

These functions are complementary to filtering on ranks. Filtering gives you all variables, with each observation in a separate row:

```{r}
not_cancelled %>% 
  group_by(year,month,day) %>% 
  mutate(r=min_rank(desc(dep_time))) %>% 
  filter(r %in% range(r))
```

* Counts: You’ve seen n(), which takes no arguments, and returns the size of the current group. To count the number of non-missing values, use sum(!is.na(x)). To count the number of distinct (unique) values, use n_distinct(x)

```{r}
# Which destinations have the most carriers?
not_cancelled %>% 
  group_by(dest) %>% 
  summarise(carriers=n_distinct(carrier)) %>% 
  arrange(desc(carriers))
```

Counts are so useful that dplyr provides a simple helper if all you want is a count:

```{r}
not_cancelled %>% 
  count(dest)
```

You can optionally provide a weight variable. For example, you could use this to “count” (sum) the total number of miles a plane flew:

```{r}
not_cancelled %>% 
  count(tailnum,wt=distance)
```

Counts and proportions of logical values: sum(x > 10), mean(y == 0). When used with numeric functions, TRUE is converted to 1 and FALSE to 0. This makes sum() and mean() very useful: sum(x) gives the number of TRUEs in x, and mean(x) gives the proportion.

```{r}
# How many flights left before 5am? These usually indicate delayed flights from the previous day.
not_cancelled %>% 
  group_by(year,month,day) %>% 
  summarise(n_early=sum(dep_time<500))

# What proportion of flights are delayed by more than an hour?
not_cancelled %>% 
  group_by(year,month,day) %>% 
  summarise(hour_perc=mean(arr_delay>60))
```

## Grouping by multiple variables

When you group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset:

```{r}
daily <- group_by(flights,year,month,day)
(per_day <- summarise(daily,flights=n()))

(per_month <- summarise(per_day,flights=sum(flights)))

(per_year <- summarise(per_month,flights=sum(flights)))
```

Be careful when progressively rolling up summaries: it’s OK for sums and counts, but you need to think about weighting means and variances, and it’s not possible to do it exactly for rank-based statistics like the median. In other words, the sum of groupwise sums is the overall sum, but the median of groupwise medians is not the overall median.

## Ungrouping

If you need to remove grouping, and return to operations on ungrouped data, use ungroup().

```{r}
daily <- group_by(flights,year,month,day)

daily %>% 
  summarise(flights=n())

daily %>% 
  ungroup() %>% 
  summarise(flights=n())
```

# Grouped mutates

Grouping is most useful in conjunction with summarise(), but you can also do convenient operations with mutate() and filter():

* Find the worst members of each group:

```{r}
test <- flights_sml %>% 
  group_by(year,month,day) %>% 
  filter(rank(desc(arr_delay))<10)
```

* Find all groups bigger than a threshold:

```{r}
popular_dests <- flights %>% 
  group_by(dest) %>% 
  filter(n()>365)
popular_dests
```

# Standardise to compute per group metrics:

```{r}
popular_dests %>% 
  filter(arr_delay>0) %>% 
  mutate(prop_delay=arr_delay/sum(arr_delay)) %>% 
  select(year:day,dest,arr_delay,prop_delay)
```

A grouped filter is a grouped mutate followed by an ungrouped filter. I generally avoid them except for quick and dirty manipulations: otherwise it’s hard to check that you’ve done the manipulation correctly.

Functions that work most naturally in grouped mutates and filters are known as window functions (vs. the summary functions used for summaries). You can learn more about useful window functions in the corresponding vignette: vignette("window-functions").