---
title: "Model basics"
author: "Xiaochi"
date: "26/08/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = TRUE,
	warning = TRUE
)
library(tidyverse)
library(modelr)
options(na.action = na.warn)
```

The goal of a model is to provide a simple low-dimensional summary of a dataset. In the context of this book we’re going to use models to partition data into patterns and residuals. Strong patterns will hide subtler trends, so we’ll use models to help peel back layers of structure as we explore a dataset.

There are two parts to a model:

1. define a family of models: express a precise, but generic, pattern that you want to capture.
    + straight line: y = a_1 * x + a_2
    + quadratic curve: y = a_1 * x ^ a_2

2. generate a fitted model: finding the model from the family that is the closest to your data. This takes the generic model family and makes it specific.
    + y = 3 * x + 7
    + y = 9 * x ^ 2

It’s important to understand that a fitted model is just the closest model from a family of models. That implies that you have the “best” model (according to some criteria); it doesn’t imply that you have a good model and it certainly doesn’t imply that the model is “true”. George Box puts this well in his famous aphorism:

All models are wrong, but some are useful. The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful.

# A simple model

sim1 contains two continuous variables, x and y. Let's plot them to see hwo they're related:

```{r}
sim1

ggplot(sim1, aes(x, y))+
  geom_point()
```

You can see a strong pattern in the data. Let’s use a model to capture that pattern and make it explicit. 

It’s our job to supply the basic form of the model. In this case, the relationship looks linear, i.e. y = a_0 + a_1 * x.

Let’s start by getting a feel for what models from that family look like by randomly generating a few and overlaying them on the data. For this simple case, we can use geom_abline() which takes a slope and intercept as parameters.

```{r}
# models are defined by a1 and a2
# a1 is intercept, a2 is slope
# totally 250 models
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

models

ggplot(sim1, aes(x, y))+
  geom_abline(aes(intercept = a1, slope = a2),
              data = models,
              alpha = 1/4)+
  geom_point()
```

There are 250 models on this plot, but a lot are really bad! We need to find the good models by making precise our intuition that a good model is “close” to the data. 

We need a way to quantify the distance between the data and a model. Then we can fit the model by finding the value of a_1 and a_2 that generate the model with the smallest distance from this data.

One easy place to start is to find the vertical distance between each point and the model. This distance is just the difference between the y value given by the model (the prediction), and the actual y value in the data (the response).

## To compute this distance:

1. we first turn our model family into an R function: 

* input
    + model parameters
    + data
* output
    + values predicted by the model

```{r}
# turn our model family into an R function
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

model1(c(7, 1.5), sim1)
```

2. Next, we need some way to compute an overall distance between the predicted and actual values. 

In other words, the plot above shows 30 distances: how do we collapse that into a single number?

One common way to do this in statistics to use the “root-mean-squared deviation”: 

1. we compute the difference between actual and predicted, 
2. then square them
3. then average them
4. then take the square root

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

# residual = actual - predicted
# predicted need model parameter and data
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

measure_distance(c(7, 1.5), sim1)
```

3. Now we can use purrr to compute the distance for all 250 models defined above. 

We need a helper function because our distance function expects the model as a numeric vector of length 2.

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

# helper function
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models
```

4. Next, let’s overlay the 10 best models on to the data. 

I’ve coloured the models by -dist: this is an easy way to make sure that the best models (i.e. the ones with the smallest distance) get the brighest colours.

```{r}
ggplot(sim1, aes(x, y))+
  geom_point(size = 2, colour = "grey30")+
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist),
    data = filter(models, rank(dist) <=10))
```

## Models as observations

We can also think about these models as observations, and visualising with a scatterplot of a1 vs a2, again coloured by -dist. 

We can no longer directly see how the model compares to the data, but we can see many models at once. 

Again, I’ve highlighted the 10 best models, this time by drawing red circles underneath them.

```{r}
ggplot(models, aes(a1, a2))+
  geom_point(data = filter(models, rank(dist) <= 10),
             size = 4,
             colour = "red") +
  geom_point(aes(colour = -dist))
```

Instead of trying lots of random models, we could be more systematic and generate an evenly spaced grid of points (this is called a grid search). I picked the parameters of the grid roughly by looking at where the best models were in the plot above.

```{r}
grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid

grid %>% 
  ggplot(aes(a1, a2))+
  geom_point(data = filter(grid, rank(dist) <= 10),
             size = 4,
             colour = "red") + 
  geom_point(aes(colour = -dist))
```

When you overlay the best 10 models back on the original data, they all look pretty good:

```{r}
ggplot(sim1, aes(x, y))+
  geom_point(size = 2, colour = "grey30") +
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist),
    data = filter(grid, rank(dist) <= 10)
  )
```

## Find the best model

You could imagine iteratively making the grid finer and finer until you narrowed in on the best model. But there’s a better way to tackle that problem: a numerical minimisation tool called Newton-Raphson search.

The intuition of Newton-Raphson is pretty simple: you pick a starting point and look around for the steepest slope. You then ski down that slope a little way, and then repeat again and again, until you can’t go any lower. （梯度下降，让距离减小）

In R, we can do that with optim():

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

# find the parameters to make the return value of measure_distance minimum
best <- optim(c(0, 0), measure_distance, data = sim1)
best
best$par


ggplot(sim1, aes(x, y))+
  geom_point(size = 2, colour = "grey30") +
  geom_abline(intercept = best$par[1], slope = best$par[2])
```

Don’t worry too much about the details of how optim() works. It’s the intuition that’s important here:

1. have a function that defines the distance between a model and a dataset
2. have an algorithm that can minimise that distance by modifying the parameters of the model
3. find the best model. 

The neat thing about this approach is that it will work for any family of models that you can write an equation for.

## Alternative way

There’s one more approach that we can use for this model, because it’s a special case of a broader family: linear models. A linear model has the general form y = a_1 + a_2 * x_1 + a_3 * x_2 + ... + a_n * x_(n - 1). So this simple model is equivalent to a general linear model where n is 2 and x_1 is x.

R has a tool specifically designed for fitting linear models called lm().

lm() has a special way to specify the model family: formulas.

Formulas look like y ~ x, which lm() will translate to a function like y = a_1 + a_2 * x. We can fit the model and look at the output

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

These are exactly the same values we got with optim()! 

Behind the scenes lm() doesn’t use optim() but instead takes advantage of the mathematical structure of linear models. Using some connections between geometry, calculus, and linear algebra, lm() actually finds the closest model in a single step, using a sophisticated algorithm. This approach is both faster, and guarantees that there is a global minimum.

# Visualising models

For simple models, like the one above, you can figure out what pattern the model captures by carefully studying the model family and the fitted coefficients. And if you ever take a statistics course on modelling, you’re likely to spend a lot of time doing just that. Here, however, we’re going to take a different tack.

* Here, we’re going to focus on understanding a model by looking at its predictions.
    * This has a big advantage: every type of predictive model makes predictions (otherwise what use would it be?) so we can use the same set of techniques to understand any type of predictive model.

* It’s also useful to see what the model doesn’t capture, the so-called residuals which are left after subtracting the predictions from the data.
    * Residuals are powerful because they allow us to use models to remove striking patterns so we can study the subtler trends that remain.


## Predictions

To visualise the predictions from a model:

1. generating an evenly spaced grid of values that covers the region where our data lies. 
    * modelr::data_grid(). 
    * Its first argument is a data frame
    * for each subsequent argument, it finds the unique variables
    * generates all combinations:

```{r}
grid <- sim1 %>% 
  data_grid(x)
grid
```

2. Next we add predictions. 
    * modelr::add_predictions()
    * takes a data frame and a model. 
    * It adds the predictions from the model to a new column in the data frame:

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)

grid <- grid %>% 
  add_predictions(sim1_mod)
grid
```

3. Next, we plot the predictions. 

```{r}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)
```

You might wonder about all this extra work compared to just using geom_abline(). But the advantage of this approach is that it will work with any model in R, from the simplest to the most complex. You’re only limited by your visualisation skills.

## Residuals

The flip-side of predictions are residuals.

* The predictions tells you the pattern that the model has captured
* The residuals tell you waht the model has missed.

The residuals are just the distances between the observed and predicted values that we computed above.

$$residual = observed - predicted$$

We add residuals to the data with add_residuals(), which works much like add_predictions(). 

Note, however, that we use the original dataset, not a manufactured grid. This is because to compute residuals we need actual y values.

```{r}
sim1 <- sim1 %>% 
  add_residuals(sim1_mod)
sim1
```

There are a few different ways to understand what the residuals tell us about the model.

1. One way is to simply draw a frequency polygon to help us understand the spread of the residuals:

```{r}
ggplot(sim1, aes(resid)) +
  geom_freqpoly(binwidth = 0.5)
```

This helps you calibrate the quality of the model: how far away are the predictions from the observed values? Note that the average of the residual will always be 0.

You’ll often want to recreate plots using the residuals instead of the original predictor. You’ll see a lot of that in the next chapter.

```{r}
ggplot(sim1, aes(x, resid)) +
  geom_ref_line(h = 0) +
  geom_point()
```

This looks like random noise, suggesting that our model has done a good job of capturing the patterns in the dataset.

# Formulas and model families

In R, formulas provide a general way of getting “special behaviour”. Rather than evaluating the values of the variables right away, they capture them so they can be interpreted by the function.

The majority of modelling functions in R use a standard conversion from formulas to functions. 

You’ve seen one simple conversion already: y ~ x is translated to y = a_1 + a_2 * x. If you want to see what R actually does, you can use the model_matrix() function:

* input:
    * a data frame
    * a formula
* output:
    * a tibble -- defines the model equation: 
    each column in the output is associated with one coefficient in the model, the function is always y = a_1 * out_1 + a_2 * out_2 + ...

```{r}
df <- tribble(
  ~y, ~x1, ~x2,
  4, 2, 5,
  5, 1, 6
)
df
```

```{r}
# get data
model_matrix(df, y ~ x1)
```


```{r}
# get coefficient
lm(y ~ x1, data = df)
```

The way that R adds the intercept to the model is just by having a column that is full of ones. By default, R will always add this column. If you don’t want, you need to explicitly drop it with -1:

```{r}
model_matrix(df, y ~ x1 - 1)
```

The model matrix grows in an unsurprising way when you add more variables to the the model:

```{r}
df
model_matrix(df, y ~ x1 + x2)
```

```{r}
model_matrix(df, y ~ x1 + x2 -1)
```


This formula notation is sometimes called “Wilkinson-Rogers notation”, and was initially described in Symbolic Description of Factorial Models for Analysis of Variance, by G. N. Wilkinson and C. E. Rogers https://www.jstor.org/stable/2346786. It’s worth digging up and reading the original paper if you’d like to understand the full details of the modelling algebra.

The following sections expand on how this formula notation works for categorical variables, interactions, and transformation.

## Categorical variables

Imagine you have a formula like y ~ sex, where sex could either be male or female. 

It doesn’t make sense to convert that to a formula like y = x_0 + x_1 * sex because sex isn’t a number - you can’t multiply it! 

Instead what R does is convert it to y = x_0 + x_1 * sexmale where sexmale is 1 if sex is male and 0 otherwise:

```{r}
df <- tribble(
  ~ sex, ~ response,
  "male", 1,
  "female", 2,
  "male", 1
)
df
```

```{r}
model_matrix(df, response ~ sex)
```

You might wonder why R also doesn’t create a sexfemale column. The problem is that would create a column that is perfectly predictable based on the other columns (i.e. sexfemale = 1 - sexmale). Unfortunately the exact details of why this is a problem is beyond the scope of this book, but basically it creates a model family that is too flexible, and will have infinitely many models that are equally close to the data.

Fortunately, however, if you focus on visualising predictions you don’t need to worry about the exact parameterisation. Let’s look at some data and models to make that concrete. Here’s the sim2 dataset from modelr:

```{r}
sim2
```

```{r}
ggplot(sim2) +
  geom_point(aes(x, y))
```

We can fit a model to it, and generate predictions:

```{r}
mod2 <- lm(y ~ x, data = sim2)

grid <- sim2 %>% 
  data_grid(x) %>% 
  add_predictions(mod2)

grid
```

Effectively, a model with a categorical x will predict the mean value for each category. (Why? Because the mean minimises the root-mean-squared distance.) That’s easy to see if we overlay the predictions on top of the original data:

```{r}
ggplot(sim2, aes(x)) +
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), colour = "red", size = 4)
```

You can’t make predictions about levels that you didn’t observe. Sometimes you’ll do this by accident so it’s good to recognise this error message:

```{r error=TRUE}
tibble(x = "e") %>% 
  add_predictions(mod2)
```


## Interactions (continuous and categorical)

What happens when you combine a continuous and a categorical variable? 

sim3 contains a categorical predictor and a continuous predictor. 

We can visualise it with a simple plot:

```{r}
sim3
```

```{r}
ggplot(sim3, aes(x1, y)) +
  geom_point(aes(colour = x2))
```

There are two possible models you could fit to this data:

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod1
```


```{r}
model_matrix(sim3, y ~ x1 + x2)
```

```{r}
mod2 <- lm(y ~ x1 * x2, data = sim3)
mod2
```


```{r}
model_matrix(sim3, y ~ x1 * x2)
```

* +: the model will estimate each effect independent of all the others.

* *: It’s possible to fit the so-called interaction. 

For example, y ~ x1 * x2 is translated to y = a_0 + a_1 * x1 + a_2 * x2 + a_12 * x1 * x2. Note that whenever you use *, both the interaction and the individual components are included in the model.

To visualise these models we need two new tricks:

1. We have two predictors, so we need to give data_grid() both variables. It finds all the unique values of x1 and x2 and then generates all combinations.

```{r}
grid <- sim3 %>% 
  data_grid(x1, x2)
grid
```

2. To generate predictions from both models simultaneously, we can use gather_predictions() which adds each prediction as a row. The complement of gather_predictions() is spread_predictions() which adds each prediction to a new column.

```{r}
grid <- sim3 %>% 
  data_grid(x1, x2) %>% 
  gather_predictions(mod1, mod2)
grid
```

We can visualise the results for both models on one plot using facetting:

```{r}
ggplot(sim3, aes(x1, y, colour = x2)) +
  geom_point() +
  geom_line(data = grid, aes(y = pred)) +
  facet_wrap(~ model)
```

Note that the model that uses + has the same slope for each line, but different intercepts. The model that uses * has a different slope and intercept for each line.

Which model is better for this data? We can take look at the residuals. 

Here I’ve facetted by both model and x2 because it makes it easier to see the pattern within each group.

```{r}
sim3

sim3_resid <- sim3 %>% 
  gather_residuals(mod1, mod2)

sim3_resid

ggplot(sim3_resid, aes(x1, resid, colour = x2)) +
  geom_point() +
  facet_grid(model ~ x2)
```

There is little obvious pattern in the residuals for mod2. The residuals for mod1 show that the model has clearly missed some pattern in b, and less so, but still present is pattern in c, and d.

You might wonder if there’s a precise way to tell which of mod1 or mod2 is better. There is, but it requires a lot of mathematical background, and we don’t really care. Here, we’re interested in a qualitative assessment of whether or not the model has captured the pattern that we’re interested in.

## Interactions (two continuous)

Let’s take a look at the equivalent model for two continuous variables. Initially things proceed almost identically to the previous example:

```{r}
sim4
```

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod1
```

```{r}
mod2 <- lm(y ~ x1 * x2, data = sim4)
mod2
```

```{r}
grid_x1_x2 <- sim4 %>% 
  data_grid(
    x1 = seq_range(x1, 5),
    x2 = seq_range(x2, 5)
    ) %>% 
  gather_predictions(mod1, mod2)
grid_x1_x2  
```

Note my use of seq_range() inside data_grid(). Instead of using every unique value of x, I’m going to use a regularly spaced grid of five values between the minimum and maximum numbers.

There are two other useful arguments to seq_range():

* pretty = TRUE will generate a “pretty” sequence, i.e. something that looks nice to the human eye. This is useful if you want to produce tables of output:

```{r}
seq_range(c(0.0123, 0.923423), n = 5)
```

```{r}
seq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE)
```

* trim = 0.1 will trim off 10% of the tail values. This is useful if the variables have a long tailed distribution and you want to focus on generating values near the center:

```{r}
x1 <- rcauchy(100)
x1 <- tibble(x1)
ggplot(x1, aes(x1)) +
  geom_freqpoly()
```

```{r}
x1 <- x1$x1
```


```{r}
seq_range(x1, n = 5)
seq_range(x1, n = 5, trim = 0.1)
seq_range(x1, n = 5, trim = 0.25)
seq_range(x1, n = 5, trim = 0.50)
```

* expand = 0.1 is in some sense the opposite of trim() it expands the range by 10%.

```{r}
x2 <- c(0, 1)
seq_range(x2, n = 5)
seq_range(x2, n = 5, expand = 0.1)
seq_range(x2, n = 5, expand = 0.25)
seq_range(x2, n = 5, expand = 0.5)
```

Next let’s try and visualise that model. We have two continuous predictors, so you can imagine the model like a 3d surface. We could display that using geom_tile():

```{r}
ggplot(grid_x1_x2, aes(x1, x2)) +
  geom_tile(aes(fill = pred)) +
  facet_wrap(~ model)
```

That doesn’t suggest that the models are very different! But that’s partly an illusion: our eyes and brains are not very good at accurately comparing shades of colour. 

Instead of looking at the surface from the top, we could look at it from either side, showing multiple slices:

```{r}
ggplot(grid_x1_x2, aes(x1, pred, colour = x2, group = x2)) +
  geom_line() +
  facet_wrap(~ model)
```


```{r}
ggplot(grid_x1_x2, aes(x2, pred, colour = x1, group = x1)) +
  geom_line() +
  facet_wrap(~ model)
```

This shows you that interaction between two continuous variables works basically the same way as for a categorical and continuous variable. An interaction says that there’s not a fixed offset: you need to consider both values of x1 and x2 simultaneously in order to predict y.

You can see that even with just two continuous variables, coming up with good visualisations are hard. But that’s reasonable: you shouldn’t expect it will be easy to understand how three or more variables simultaneously interact! But again, we’re saved a little because we’re using models for exploration, and you can gradually build up your model over time. The model doesn’t have to be perfect, it just has to help you reveal a little more about your data.

## Transformations

You can also perform transformations inside the model formula.

If your transformation involves +, *, ^, or -, you’ll need to wrap it in I() so R doesn’t treat it like part of the model specification.

* y ~ I(x ^ 2) + x ---> y = a_1 + a_2 * x + a_3 * x^2
* y ~ x ^ 2 + x --->  y ~ x * x + x ---> y = a_1 + a_2 * x

Again, if you get confused about what your model is doing, you can always use model_matrix() to see exactly what equation lm() is fitting:

```{r}
df <- tribble(
  ~y, ~x,
  1, 1,
  2, 2,
  3, 3
)

df
```

```{r}
model_matrix(df, y ~ x^2 + x)
```

```{r}
model_matrix(df, y ~ I(x^2) + x)
```

Transformations are useful because you can use them to approximate non-linear functions. 

If you’ve taken a calculus class, you may have heard of Taylor’s theorem which says you can approximate any smooth function with an infinite sum of polynomials. 

That means you can use a polynomial function to get arbitrarily close to a smooth function by fitting an equation like $y = a_0 + a_1 * x + a_2 * x^2 + a_3 * x^3 + a_4*x^4+...$. 

Typing that sequence by hand is tedious, so R provides a helper function: poly():

```{r}
model_matrix(df, y ~ poly(x, 2))
```

However there’s one major problem with using poly(): outside the range of the data, polynomials rapidly shoot off to positive or negative infinity. One safer alternative is to use the natural spline, splines::ns().

```{r}
library(splines)
df
model_matrix(df, y ~ ns(x, 2))
```

Let’s see what that looks like when we try and approximate a non-linear function:

```{r}
sim5 <- tibble(
  x = seq(0, 3.5 * pi, length = 50),
  y = 4 * sin(x) + rnorm(length(x))
)
sim5
```

```{r}
ggplot(sim5, aes(x, y)) +
  geom_point()
```

I’m going to fit five models to this data.

```{r}
mod1 <- lm(y ~ ns(x, 1), data = sim5)
model_matrix(sim5, y ~ ns(x, 1))

model1 <- lm(y ~ poly(x, 1), data = sim5)
model_matrix(sim5, y ~ poly(x, 1))

mod2 <- lm(y ~ ns(x, 2), data = sim5)
model_matrix(sim5, y ~ ns(x, 2))

model2 <- lm(y ~ poly(x, 2), data = sim5)
model_matrix(sim5, y ~ poly(x, 2))

mod3 <- lm(y ~ ns(x, 3), data = sim5)
model_matrix(sim5, y ~ ns(x, 3))

model3 <- lm(y ~ poly(x, 3), data = sim5)
model_matrix(sim5, y ~ poly(x, 3))

mod4 <- lm(y ~ ns(x, 4), data = sim5)
model_matrix(sim5, y ~ ns(x, 4))

model4 <- lm(y ~ poly(x, 4), data = sim5)
model_matrix(sim5, y ~ poly(x, 4))

mod5 <- lm(y ~ ns(x, 5), data = sim5)
model_matrix(sim5, y ~ ns(x, 5))

model5 <- lm(y ~ poly(x, 5), data = sim5)
model_matrix(sim5, y ~ poly(x, 5))
```

```{r}
grid_x <- sim5 %>% 
  data_grid(x = seq_range(x, n = 50, expand = 0.1)) %>% 
  gather_predictions(mod1, model1, 
                     mod2, model2,
                     mod3, model3,
                     mod4, model4,
                     mod5, model5) 

grid_x <- grid_x%>% 
  mutate(model = factor(model, levels = unique(grid_x$model)))
```

```{r}
ggplot(sim5, aes(x, y)) +
  geom_point() +
  geom_line(aes(y = pred), data = grid_x, colour = "red") +
  facet_wrap(~ model)
```

Notice that the extrapolation outside the range of the data is clearly bad. This is the downside to approximating a function with a polynomial. 

But this is a very real problem with every model: the model can never tell you if the behaviour is true when you start extrapolating outside the range of the data that you have seen. You must rely on theory and science.

# Missing values

Missing values obviously can not convey any information about the relationship between the variables, so modelling functions will drop any rows that contain missing values. R’s default behaviour is to silently drop them, but options(na.action = na.warn) (run in the prerequisites), makes sure you get a warning.

```{r}
df <- tribble(
  ~x, ~y,
  1, 2.2,
  2, NA,
  3, 3.5,
  4, 8.3,
  NA, 10
)
df
```

```{r}
mod <- lm(y ~ x, data = df)
```

To suppress the warning, set na.action = na.exclude:

```{r}
mod <- lm(y ~ x, data = df, na.action = na.exclude)
```

You can always see exactly how many observations were used with nobs():

```{r}
nobs(mod)
```

# Other model families

This chapter has focussed exclusively on the class of linear models, which assume a relationship of the form y = a_1 * x1 + a_2 * x2 + ... + a_n * xn.

Linear models additionally assume that the residuals have a normal distribution, which we haven’t talked about.

There are a large set of model classes that extend the linear model in various interesting ways. Some of them are:

* Generalised linear models, e.g. stats::glm()
    * Linear models assume that the response is continuous and the error has a normal distribution.
    * Generalised linear models extend linear models to include non-continuous responses (e.g. binary data or counts). 
    * They work by defining a distance metric based on the statistical idea of likelihood.

* Generalised additive models, e.g. mgcv::gam()
    * extend generalised linear models to incorporate arbitrary smooth functions.
    * write a formula like y ~ s(x) which becomes an equation like y = f(x)
    * gam() estimate what that function is (subject to some smoothness constraints to make the problem tractable).

```{r}
sim5
mod <- mgcv::gam(y ~ s(x), data = sim5)
summary(mod)
```


* Penalised linear models, e.g. glmnet::glmnet()
    * add a penalty term to the distance that penalises complex models (as defined by the distance between the parameter vector and the origin). 
    * This tends to make models that generalise better to new datasets from the same population.

* Robust linear models, e.g. MASS:rlm()
    * tweak the distance to downweight points that are very far away. 
    * This makes them less sensitive to the presence of outliers, at the cost of being not quite as good when there are no outliers.

* Trees, e.g. rpart::rpart()
    * attack the problem in a completely different way than linear models. 
    * They fit a piece-wise constant model, splitting the data into progressively smaller and smaller pieces. 
    * Trees aren’t terribly effective by themselves, but they are very powerful when used in aggregate by models like random forests (e.g. randomForest::randomForest()) or gradient boosting machines (e.g. xgboost::xgboost.)

These models all work similarly from a programming perspective. Once you’ve mastered linear models, you should find it easy to master the mechanics of these other model classes. 

Being a skilled modeller is a mixture of some good general principles and having a big toolbox of techniques.