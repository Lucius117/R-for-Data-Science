---
title: "Statistical Modeling"
author: "Xiaochi"
date: "18/09/2019"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(mosaic)
require(mosaicData)
```

# Chapter 1 Introduction

```{r}
sqrt(25)
```

```{r}
seq(3, 10)
```

```{r}
seq(10,3)
```

```{r}
seq(3,10,by=2)
```

```{r}
date()
```

```{r}
x <- 16
x
```

```{r}
x <- seq(1,7)
x
```

# Chapter 2 Data in R

```{r}
head(Galton)
```

```{r}
Swim <- read.csv("http://tiny.cc/mosaic/swim100m.csv")
```

```{r}
names(Swim)
```

```{r}
summary(Swim)
```

```{r}
mean(Swim$time)
```

```{r}
mean( ~ time, data = Swim)
```

```{r}
min( ~ time, data = Swim)
```

```{r}
mean(time ~ sex, data = Swim )
```

```{r}
mean( Swim$time ~ Swim$sex )
```

```{r}
Swim$minutes = Swim$time/60
```

```{r}
head(Swim, n = 3L)
```

```{r}
Swim$time = Swim$time/60
head(Swim, n = 3L)
```

```{r}
Kids <- read.csv("http://tiny.cc/mosaic/kidsfeet.csv")
nrow(Kids)
```

```{r}
deal(Kids, 5)
```

```{r}
resample(Kids, 5)
```

# Chapter 3 Describing Variation

```{r}
Galton = read.csv("http://tiny.cc/mosaic/galton.csv")
```

```{r}
mean( ~ height, data = Galton)
```

```{r}
median( ~ height, data = Galton)
```

```{r}
sd( ~ height, data = Galton)
```

```{r}
var( ~ height, data = Galton)
```

```{r}
mean(Galton$height)
```

```{r}
median(Galton$height)
```

```{r}
median(height ~ sex, data = Galton)
```

```{r}
pdata( ~ height, 63, data = Galton)
```

```{r}
qdata( ~ height, 0.2, data = Galton)
```

```{r}
qdata( ~ height, 0.5, data = Galton)
```

```{r}
median( ~ height, data = Galton)
```

```{r}
qdata( ~ height, c(0.25, 0.75), data = Galton)
```

```{r}
qdata( ~ height, c(0.025, 0.975), data = Galton)
```

```{r}
quantile(Galton$height, c(0.025, 0.975))
```

```{r}
IQR( ~ height, data = Galton)
```

```{r}
histogram( ~ height, data = Galton)
```

```{r}
histogram( ~ height, data = Galton, breaks = 25 )
```

```{r}
histogram( ~ height, data = Galton, type = "count")
```

```{r}
histogram( ~ height, data = Galton, type = "density",
     xlab="Height (inches)", 
     main="Distribution of Heights",
     col="gray")
```

```{r}
densityplot( ~ height, data = Galton) 
```

```{r}
densityplot( ~ height, data = Galton, plot.points = FALSE)
```

```{r}
bwplot( ~ height, data = Galton)
```

```{r}
bwplot(height ~ sex, data = Galton)
```

```{r}
tally( ~ sex, data = Galton)
```

```{r}
tally( ~ sex, data = Galton)/nrow(Galton)
```

# Chapter 4 Groupwise models

```{r}
Runners = read.csv("http://tiny.cc/mosaic/Cherry-Blossom-2008.csv")
```

```{r}
mean( gun ~ 1, data = Runners)
```


```{r}
median(gun ~ 1, data = Runners )
```

```{r}
sd( gun ~ 1, data = Runners )
```

```{r}
mean(gun ~ sex, data = Runners)
```

```{r}
sd( gun ~ sex, data = Runners )
```

```{r}
CPS = read.csv("http://tiny.cc/mosaic/cps.csv")
```

```{r}
mean( wage ~ sector, data = CPS )
```

```{r}
W <- read.csv("http://tiny.cc/mosaic/whickham.csv")
```

```{r}
levels(W$outcome)
```

```{r}
W$ageGroups <- cut(W$age, breaks=c(0,30,40,53,64,75,100))
```

```{r}
Kids <- KidsFeet   # just get this from mosaicData instead of going out on the web with read.csv
```

# Chapter 5 Confidence intervals

```{r}
Runners = read.csv("http://tiny.cc/mosaic/Cherry-Blossom-2008.csv")
names( Runners )
```

```{r}
Mysamp = deal( Runners, 100 )
```

```{r}
mean( gun ~ sex, data = Mysamp )
```

```{r}
nums = c(1,2,3,4,5)
nums
```

```{r}
resample(nums)
```

```{r}
mean( gun ~ sex, data = resample(Mysamp) )
```

```{r}
mean( gun ~ sex, data = resample(Mysamp) )
```

```{r}
do(5) * mean( gun ~ sex, data = resample(Mysamp) )
```

```{r}
trials = do(500) * mean( gun ~ sex, data = resample(Mysamp) )
head(trials)
```

```{r}
confint(trials)
```

```{r}
histogram(trials$F)
```

```{r}
trials = do(500) * mean( gun ~ sex, data = deal(Runners, 100) )
confint(trials)
```

```{r}
Grades = read.csv("http://tiny.cc/mosaic/grades.csv")
gp = read.csv("http://tiny.cc/mosaic/grade-to-number.csv")
all.students = merge(Grades, gp)
one.student = subset( all.students, sid=="S31509" )
one.student
```

```{r}
mean( ~ gradepoint, data=one.student )
```

```{r}
GPAs <- mean( gradepoint ~ sid, data=all.students )
head(GPAs)
```

```{r}
trials = do(100)*mean( ~ gradepoint, data=resample(one.student), na.rm = TRUE )
```

```{r}
confint(trials)
```

# Chapter 6 Language of models

function
* formula
* graphs
* tables

$$residuals = actual - model$$

the models (shown as smooth curves) suggest that wages increase until age 40, then level off.

wages tend to increase wiht age, up until about age 40, and they do so differently for men and for women and differently for married people and single people.

how gas usage depends on temperature

how wages depend on sex or marital status or age

gas usage can be expected to go down by 4 ccf for every degree of temperature increase

For the model of wages in Figure 6.4, the slope for single females is about 0.20 dollars-per-hour/year: for every year older a single female is, wages tipically go up by 20 cents-per-hour.

intercept term

main terms

interaction terms

transformation terms

Model terms are always about how the response variable depends on the explanatory variables, not how explanatory variables depend on one another.

An interaction term between two varaibles describes how two explanatory variables combine jointly to influence the response variable.

model design
* response variable
* explanatory variables
* model terms




```{r}
Utils <- Utilities
```

```{r}
CPS <- CPS85
```

plot the response variable on the vertical axis and an explanatory variable on the horizontal axis.

## Quantitative Explanatory Variable

When the explanatory variable is quantitative, a scatter-plot is an appropriate graphical format. In the scatter plot, each case is a single point.

```{r}
xyplot( ccf ~ temp, data = Utils)
```

The first argument is a model formula written using the tilde modeling notation. This formula, ccf ~ temp is pronounced “ccf versus temperature.” 


```{r}
xyplot( ccf ~ temp, data=Utils, 
      xlab = "Temperature (deg F)",
      ylab = "Natural Gas Usage (ccf)")
```

## Categorical Explanatory Variable

When the explanatory variable is categorical, an appropriate format of display is the box-and-whiskers plot, made with the bwplot operator. Here, for example, is the wage versus sex from the Current Population Survey:

```{r}
bwplot( wage ~ sex, data=CPS)
```

```{r}
bwplot(wage~sector, data=CPS, ylim=c(0,30))
```

```{r}
densityplot(~ wage, groups = sex, data = CPS, auto.key = TRUE)
```

## Multiple Explanatory Variables

* Coding the additional explanatory variable] using color or symbol shapes. This is done by using the groups argument set to the name of the additional explanatory variable. For example:

```{r}
xyplot( wage ~ age, groups = sex, data = CPS, auto.key = TRUE)
```

* Splitting the plot into the groups defined by the additional explanatory variable. This is done by including the additional variable in the model formula using a | separator.

```{r}
xyplot( wage ~ age | sex, data = CPS)
```

## Fitting Models and Finding Model Values

The lm operator (short for “Linear Model”) will translate a model design into fitted model values.

```{r}
Swim <- read.csv("http://tiny.cc/mosaic/swim100m.csv")
```

```{r}
mod1 <- lm(time ~ 1, data = Swim)
```

Once the model has been constructed, the fitted values can be found using the fitted operator:

```{r}
fitted(mod1)
```

Plot out both the data values and the model values versus year just to emphasize that the model values are the same for every case:

```{r}
xyplot(time + fitted(mod1) ~ year, data = Swim)
```


```{r}
mod2 <- lm( time ~ 1 + year, data = Swim)
xyplot(time + fitted(mod2) ~ year, data = Swim, auto.key = TRUE)
```

```{r}
mod3 <- lm( time ~ 1 + sex, data = Swim)
xyplot(time + fitted(mod3) ~ year, data = Swim, auto.key = TRUE)
```

```{r}
mod4 <- lm( time ~ 1 + sex + year, data = Swim)
xyplot(time + fitted(mod4) ~ year, data = Swim, auto.key = TRUE)
```


```{r}
mod5 <- lm( time ~ 1 + year + sex + year:sex, data = Swim)
xyplot(time + fitted(mod5) ~ year, data = Swim, auto.key = TRUE)
```

```{r}
mod7 <- lm( time ~ year + I(year^2) + sex, data = Swim)
xyplot(time + fitted(mod7) ~ year, data = Swim, auto.key = TRUE)
```

```{r}
mod7 <- lm( time ~ poly(year, 2) + sex, data = Swim)
xyplot(time + fitted(mod7) ~ year, data = Swim, auto.key = TRUE)
```


# Chapter 7 Model formulas and coefficients

model design

model formula: takes each of the terms in the model design and multiplies it by a number, model coefficient. Each of the terms has its own coefficient.

an extra inch of mother's height is associated with an extra 0.283 inches in the child

$$height = 132.3 - 1.43mother - 1.21father + 0.0247father*mother$$

$$height = 15.3 + 0.322mother + 0.406father + 5.23sexM$$

a one-inch increase in fatehr leads to a 0.406 increase in the model value of height.

a one-inch change in mother will correspond to a 0.322 increase in the model value of height.

derivative: making a small change in the explanatory variable and seeing hwo much the model value changes.

the model formula indicates that males are, on average, 5.32 inches taller than females.

a one-year increase in the amount of education that a worker received is associated with a 74 cents per hour increase in wages.

strategy for finding effective models:
* fit a model that matches what you know about the system
* check how good the fit is
* try refining the model, adding detail by including curvy transformation terms
* check the fit again and see if the improvement in the fit goes beyond what would be expected from chance.

a reasonable strategy
* include only main terms
* add in some interaction terms
* try transformation terms

## Examining model coefficients

```{r}
SwimRecords
```

lm() operator finds model coefficients

```{r}
mod <- lm( time ~ year + sex, data = SwimRecords)
```

```{r}
coef(mod)
```

```{r}
555.7 - 0.2515*2010 - 9.798
```

the “value” used to multiply the intercept is always 1

the “value” used for a categorical level is either 0 or 1 depending on whether there is a match with the level

```{r}
summary(mod)
```

When a model includes interaction terms, the interaction coefficients need to be multiplied by all the values involved in the interaction. 

```{r}
mod2 <- lm( time ~ year * sex, data = SwimRecords)
coef(mod2)
```

```{r}
697.3 - 0.3240 * 2010 - 302.5 + 0.1499 * 2010 * 1
```

## Other Useful Operators

* cross() will combine two categorical variables into a single variable.

```{r}
CPS <- CPS85  # from mosaicData
RaceSex <- cross(CPS$sex, CPS$race)
summary(RaceSex)
```

* as.factor() will convert a quantitative variable to a categorical variable.

```{r}
utils <- read.csv("http://tiny.cc/mosaic/utilities.csv")
```


```{r}
mod1 <- lm(temp ~ month, data = utils)
xyplot(temp + fitted(mod1) ~ month, data = utils)
```

month is treated quantitatively, so the model term month produces a straight-line relationship that does not correspond well to the data.

```{r}
mod2 <- lm(temp ~ as.factor(month), data = utils)
xyplot(temp + fitted(mod2) ~ month, data = utils )
```

In the second model, month is treated categorically, allowing a more complicated model relationship. In fact, this is a groupwise model: the model values represent the mean temperature for each month.


# Chapter 8 Fitting models to data

vectors were presented as a column of numbers

arrow: a direction and a length

Computing the fitted model values and the residuals is done with the fitted and resid.


```{r}
Swim <- SwimRecords  # from mosaicData
```


```{r}
mod1 <- lm(time ~ year + sex, data = Swim)
```

Once you have constructed the mdoel, you can use fitted and resid:

```{r}
modvals <- fitted(mod1)
head(modvals)
```

```{r}
residvals <- resid(mod1)
head(residvals)
```

## Sums of Squares

```{r}
mean(fitted(mod1))
```

```{r}
var(resid(mod1))
```

```{r}
sd(resid(mod1))
```

```{r}
summary(resid(mod1))
```

Sums of squares are very important in statistics. 

```{r}
sum(Swim$time^2)
```

```{r}
sum(fitted(mod1)^2)
```

```{r}
sum(resid(mod1)^2)
```

The partitioning of variation by models is seen by the way the sum of squares of the fitted and the residuals add up to the sum of squares of the response

```{r}
sum(fitted(mod1)^2) + sum(resid(mod1)^2)
```

```{r}
sum(Swim$time^2)
```

Don't forget the squaring stage of the operation

```{r}
sum(resid(mod1))
```

```{r}
sum(resid(mod1)^2)
```

## Redundancy


```{r}
Swim$afterwar <- Swim$year - 1945
```

```{r}
mod1 <- lm( time ~ year + sex, data = Swim)
```

When the redundant variable is added in, lm successfully detects the redundancy and handles it. This is indicated by a coefficient of NA on the redundant variable.

```{r}
mod2 <- lm( time ~ year + sex + afterwar, data = Swim)
```

```{r}
mod3 <- lm( time ~ afterwar + year + sex, data = Swim)
```

```{r}
head(fitted(mod2))
```

```{r}
head(fitted(mod3))
```

Note that whenever you use a categorical variable and an intercept term in a model, there is a redundancy.


```{r}
mod <- lm(time ~ sex - 1, data = Swim)
```

```{r}
mod <- lm(time ~ sex, data = Swim)
```

# Chapter 9 Correlation and partitioning of variation

$$R^2 = \frac{variance \  of \  fitted \  model \ values}{variance \ of \ response \ values}$$

coefficient of determination

correlation coefficient

The coefficient of determination, $R^2$, compares the variation in the response variable to the variation in the fitted model value.

```{r}
Swim <- SwimRecords # from mosaicData
```

```{r}
mod <- lm(time ~ year + sex, data = Swim)
```

```{r}
var(fitted(mod)) / var(Swim$time)
```

```{r}
rsquared(mod)
```

The regression report is a standard way of summarizing models. Such a report is produced by most statistical software packages and used in many fields. The $R^2$ statistic is a standard part of the report; look at the second line from the bottom.

```{r}
summary(mod)
```

```{r}
mod2 <- lm(time ~ year, data = Swim)
```

```{r}
sqrt(rsquared(mod2))
```

```{r}
cor(Swim$time, Swim$year)
```

Keep in mind that the correlation coefficient r summarizes only the simple linear model A ~ B where B is quantitative. But the coefficient of determination,

# Chapter 10 Total and Partial Change

The term partial relationship describes a relationship with one or more
covariates being held constant.

In contrast to a partial relationship where certain variables are being held
constant, there is also a total relationship: how an explanatory variable is
related to a response variable letting those other explanatory variables
change as they will.


## Adjustment

```{r}
Cps <- CPS85
```


```{r}
mod0 <- lm( wage ~ sex, data = Cps)
summary(mod0)
```

The coefficients indicate that a typical male makes $2.12 more per hour than a typical female. (Notice that  R2=0.0422  is very small: sex explains hardly any of the person-to-person variability in wage.)

By including the variables age, educ, and sector in the model, you can adjust for these variables:

```{r}
mod1 <- lm( wage ~ age + sex + educ + sector, data = Cps)
summary(mod1)
```

The adjusted difference between the sexes is $1.94 per hour. (The  R2=0.30  from this model is considerably larger than for mod0, but still a lot of the person-to-person variation in wages has not be captured.)

It would be wrong to claim that simply including a covariate in a model guarantees that an appropriate adjustment has been made. The effectiveness of the adjustment depends on whether the model design is appropriate, for instance whether appropriate interaction terms have been included. However, it’s certainly the case that if you don’t include the covariate in the model, you have not adjusted for it.

The other approach is to subsample the data so that the levels of the covariates are approximately constant.

```{r}
small <- subset(Cps, age <=35 & age >= 30 & educ>=10 & educ <=12 & sector=="sales" )
```

```{r}
mod4 <- lm( wage ~ sex, data = small)
summary(mod4)
```

# Chapter 11 Modeling Randomness

```{r}
coin <- c("H","T")
resample(coin, 5)
```

```{r}
die <- seq(1,6)
die
```


```{r}
resample(die,2)
```


## Random Draws from Probability Models

```{r}
resample( coin, 10, prob = c(.9,.1))
```

the rnorm() function makes random draws from a normal probability distribution.

* The required argument tells how many draws to make.
* Optional, named arguments let you specify the mean and standard deviation of the particular normal distribution that you want.

```{r}
samps <- rnorm(15, mean = 1000, sd = 75)
samps
```

```{r}
mean(samps)
```

```{r}
sd(samps)
```

```{r}
samps <- rnorm(100000, mean = 1000, sd = 75)
mean( samps )
sd(samps)
```

## Standard Probability Models

R provides a large set of operators like rnorm for different probability models. All of these operators work in the same way:

* Each has a required first argument that gives the number of draws to make
* Each has an optional set of parameters that specify the particular probability distribution you want.

You are in charge of a hiring committee that is going to interview three candidates selected from a population of job applicants that is 63% female. How many of the interviewees will be female? Modeling this as random selection from the applicant pool, a binomial model is appropriate. The size of each trial is 3, the probability of being female is 63% :

```{r}
samps <- rbinom(40, size = 3, prob = 0.63)
samps
```

```{r}
table(samps)
```

You want to simulate the number of customers who come into a store over the course of an hour. The average rate is 15 per hour. To simulate a situation where customers arrive randomly, the poisson model is appropriate:

```{r}
rpois(25, lambda = 15)
```

You want to generate a simulation of the interval between earthquakes. To simulate the random intervals with a typical rate of 0.03 earthquakes per year, you would use:

```{r}
rexp( 15, rate = 0.03 )
```

## Quantiles and Coverage Intervals

You will often need to compute coverage intervals in order to describe the range of likely outcomes from a random process.



```{r}
qnorm( c(0.025, 0.975), mean = 0, sd = 1)
```

```{r}
qbinom( c(0.025, 0.975), size = 3, prob = 0.63)
```

```{r}
qpois( c(0.025, 0.975), lambda = 15)
```

```{r}
qexp( c(.025, .975), rate = 0.03)
```

```{r}
qexp( .25, rate = 0.03)
```

```{r}
samps <- rnorm(10000, mean = 0, sd = 1)
qdata( samps, c(.025, .975) )
```



## Percentiles

Percentile (e.g. pnorm) The input argument is a measured value, something that could be the output of a single draw from the probability distribution. The output is always a number between 0 and 1 — a percentile.

Quantile (e.g. qnorm) The input is a percentile, a number between 0 and 1. The output is on the scale of the measured variable.

```{r}
pnorm(670, mean = 600, sd = 100)
```

```{r}
qnorm(0.85, mean = 600, sd = 100)
```

# Chapter 12 Confidence in Models

Regression reports are generated using software you have already encountered: lm to fit a model and summary to construct the report from the fitted model.

```{r}
mod <- lm(time ~ year + sex, data = SwimRecords)
summary(mod)
```

## Confidence Intervals from Standard Errors

The confint() function provides a convenient way to calculate confidence intervals directly.

It calculates the exact multiplier (which depends somewhat on the sample size) and applies it to the standard error to produce the confidence intervals.

```{r}
mod <- lm( time ~ year + sex, data = SwimRecords)
confint(mod)
```


## Bootstrapping Confidence Intervals

```{r}
lm( time ~ year + sex, data = SwimRecords)
```

```{r}
lm( time ~ year + sex, data = resample(SwimRecords))
```

```{r}
s = do(500) * lm( time ~ year + sex, data = resample(SwimRecords))
head(s)
```

```{r}
sd(s$sexM)
```

```{r}
confint(s$sexM, method = "stderr")
```

## Prediction Confidence Intervals

```{r}
KidsFeet
```

```{r}
levels(KidsFeet$sex)
```

```{r}
mod <- lm( width ~ length + sex, data = KidsFeet)
```

```{r}
predict(mod, newdata = data.frame( length=25, sex="G" ))
```

```{r}
predict(mod, 
        newdata = data.frame( length = 25, sex = "G" ), 
        interval = "confidence")
```

```{r}
predict(mod, 
        newdata = data.frame( length = 25, sex = "G" ), 
        interval = "prediction")
```

# Chapter 13 The Logic of Hypothesis Testing

Null Hypothesis: A statement about the world that you are interested to disprove. 
The null is almost always something that is clearly relevant and not controversial: that the conventional wisdom is true or that there is no relationship between variables.
Examples: 
“The drug has no influence on blood pressure.” 
“Smaller classes do not improve school performance.”
The allowed outcomes of the hypothesis test relate only to the null:
1. Reject the null hypothesis.
2. Fail to reject the null hypothesis.

Alternative Hypothesis: A statement about the world that motivates your study and stands in contrast to the null hypothesis. 
The outcome of the hypothesis test is not informative about the alternative. 
The importance of the alternative is in setting up the study: choosing a relevant test statistic and collecting enough data.

Test Statistic: The number that you use to summarize your study. 
This might be the sample mean, a model coefficient, or some other number.

Type I Error: A wrong outcome of the hypothesis test of a particular type.
Suppose the null hypothesis were really true. 
If you rejected it, this would be an error: a type I error.


Type II Error: A wrong outcome of a different sort. 
Suppose the alternative hypothesis were really true. 
In this situation, failing to reject the null would be an error: a type II error.

Significance Level: A conditional probability. 
In the world where the null hypothesis is true, the significance is the probability of making a type I error. 
Typically, hypothesis tests are set up so that the significance level will be less than 1 in 20, that is, less than 0.05. 
One of the things that makes hypothesis testing confusing is that you do not know whether the null hypothesis is correct; it is merely assumed to be correct for the purposes of the deductive phase of the test. 
So you can’t say what is the probability of a type I error. 
Instead, the significance level is the probability of a type I error assuming that the null hypothesis is correct.
Ideally, the significance level would be zero. 
In practice, one accepts the risk of making a type I error in order to reduce the risk of making a type II error.

Power: This is a conditional probability. 
But unlike the significance, the condition is that the alternative hypothesis is true. 
The power is the probability that, in the world where the alternative is true, you will reject the null. 
Ideally, the power should be 100%, so that if the alternative really were true the null hypothesis would certainly be rejected. 
In practice, the power is less than this and sometimes much less.

p-value: This is the usual way of presenting the result of the hypothesis test. 
It is a number that summarizes how atypical the observed value of the test statistic would be in a world where the null hypothesis is true. 
The convention for rejecting the null hypothesis is p < 0.05.

# Chapter 14 Hypothesis Testing on Whole Models